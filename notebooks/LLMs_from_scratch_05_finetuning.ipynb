{"cells":[{"cell_type":"markdown","id":"12e91914-5f51-43fa-b65b-625e73b4d17b","metadata":{"id":"12e91914-5f51-43fa-b65b-625e73b4d17b"},"source":["**LLM Workshop 2024 by Sebastian Raschka**\n","\n","This code is based on *Build a Large Language Model (From Scratch)*, [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)\n","\n","- Instruction finetuning from scratch: [ch07.ipynb](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb)"]},{"cell_type":"markdown","id":"J5AgJeWfQCuk","metadata":{"id":"J5AgJeWfQCuk"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"id":"qNhatW3l5nyH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152051,"status":"ok","timestamp":1720562857736,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"qNhatW3l5nyH","outputId":"0e67d199-bf22-4aa4-94e7-969d9bb44b96"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.7/160.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.3/205.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# Requirements from: https://github.com/rasbt/LLM-workshop-2024/blob/main/requirements.txt\n","requirements = \"\"\"\n","# torch >= 2.0.1\n","# tiktoken >= 0.5.1\n","# matplotlib >= 3.7.1\n","# numpy >= 1.24.3\n","# tensorflow >= 2.15.0\n","# tqdm >= 4.66.1\n","# numpy >= 1.25, < 2.0\n","# pandas >= 2.2.1\n","# psutil >= 5.9.5\n","litgpt[all] >= 0.4.1\n","\"\"\"\n","\n","with open(\"requirements.txt\", mode=\"wt\") as f:\n","    f.write(requirements)\n","\n","%pip install -r requirements.txt --quiet"]},{"cell_type":"markdown","id":"vgtRAPoN6ixW","metadata":{"id":"vgtRAPoN6ixW"},"source":["Add dataset and image files from Sebastian Raschka's training repository"]},{"cell_type":"code","execution_count":1,"id":"C45nepme6Waq","metadata":{"executionInfo":{"elapsed":2304,"status":"ok","timestamp":1720563051622,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"C45nepme6Waq"},"outputs":[],"source":["from pathlib import Path\n","\n","import requests\n","\n","session = requests.Session()\n","with open(\"instruction-data.json\", \"wt\", encoding=\"utf-8\") as f:\n","    response = session.get(\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/instruction-data.json\")\n","    f.write(response.text)\n","\n","for img_num in range(1, 16):\n","    filepath = Path(f\"figures/{img_num:02d}.png\")\n","    if not filepath.parent.exists():\n","        filepath.parent.mkdir(parents=True, exist_ok=True)\n","    with open(filepath, mode=\"wb\") as img_file:\n","        img_file.write(session.get(f\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/figures/{img_num:02d}.png\").content)\n"]},{"cell_type":"markdown","id":"c2520ec3-722f-4f44-bdd1-885b13e7afbf","metadata":{"id":"c2520ec3-722f-4f44-bdd1-885b13e7afbf"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# 6) Instruction finetuning (part 1; intro)"]},{"cell_type":"markdown","id":"264fca98-2f9a-4193-b435-2abfa3b4142f","metadata":{"id":"264fca98-2f9a-4193-b435-2abfa3b4142f"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/06_finetuning/figures/01.png?raw=1\" width=1000px>"]},{"cell_type":"markdown","id":"8bbc68e9-75b3-41f1-ac2c-e071c3cd0813","metadata":{"id":"8bbc68e9-75b3-41f1-ac2c-e071c3cd0813"},"source":["---\n","\n","# 6.1 Introduction to instruction finetuning"]},{"cell_type":"markdown","id":"53dba24a-6805-496c-9a7f-c75e2d3527ab","metadata":{"id":"53dba24a-6805-496c-9a7f-c75e2d3527ab"},"source":["- We saw that pretraining an LLM involves a training procedure where it learns to generate one word at a time\n","- Hence, a pretrained LLM is good at text completion, but it is not good at following instructions\n","- In this last part of the workshop, we teach the LLM to follow instructions better"]},{"cell_type":"markdown","id":"18dc0535-0904-44ed-beaf-9b678292ef35","metadata":{"id":"18dc0535-0904-44ed-beaf-9b678292ef35"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/06_finetuning/figures/02.png?raw=1\" width=800px>"]},{"cell_type":"markdown","id":"5384f0cf-ef3c-4436-a5fa-59bd25649f86","metadata":{"id":"5384f0cf-ef3c-4436-a5fa-59bd25649f86"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# 6.2 Preparing a dataset for supervised instruction finetuning"]},{"cell_type":"markdown","id":"888e4f5a-cf8a-4b59-8c2c-666c3e99f174","metadata":{"id":"888e4f5a-cf8a-4b59-8c2c-666c3e99f174"},"source":["- We will work with a simple instruction dataset I prepared for this"]},{"cell_type":"code","execution_count":2,"id":"0G3axLw6kY1N","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191,"status":"ok","timestamp":1720563090025,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"0G3axLw6kY1N","outputId":"d433ca91-d8cf-422b-fb1a-a10f785c4346"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of entries: 1100\n"]}],"source":["import json\n","\n","\n","file_path = \"instruction-data.json\"\n","\n","with open(file_path, \"r\") as file:\n","    data = json.load(file)\n","print(\"Number of entries:\", len(data))"]},{"cell_type":"markdown","id":"d7af8176-4255-4e92-8c7d-998771733eb8","metadata":{"id":"d7af8176-4255-4e92-8c7d-998771733eb8"},"source":["- Each item in the `data` list we loaded from the JSON file above is a dictionary in the following form"]},{"cell_type":"code","execution_count":3,"id":"-LiuBMsHkzQV","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1720563094253,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"-LiuBMsHkzQV","outputId":"a1522d2c-641d-4db4-ca19-3399fd0db9e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Example entry:\n"," {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"]}],"source":["print(\"Example entry:\\n\", data[50])"]},{"cell_type":"markdown","id":"c5a32b34-485a-4816-a77a-da14f9fe6e46","metadata":{"id":"c5a32b34-485a-4816-a77a-da14f9fe6e46"},"source":["- Note that the `'input'` field can be empty:"]},{"cell_type":"code","execution_count":4,"id":"uFInFxDDk2Je","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1720563105739,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"uFInFxDDk2Je","outputId":"ca1cd9c7-cab4-4593-bde2-80d9c2529fca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Another example entry:\n"," {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"]}],"source":["print(\"Another example entry:\\n\", data[999])"]},{"cell_type":"markdown","id":"f034799a-6575-45fd-98c9-9d1012d0fd58","metadata":{"id":"f034799a-6575-45fd-98c9-9d1012d0fd58"},"source":["- Instruction finetuning is often referred to as \"supervised instruction finetuning\" because it involves training a model on a dataset where the input-output pairs are explicitly provided\n","- There are different ways to format the entries as inputs to the LLM; the figure below illustrates two example formats that were used for training the Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html) and Phi-3 (https://arxiv.org/abs/2404.14219) LLMs, respectively"]},{"cell_type":"markdown","id":"dffa4f70-44d4-4be4-89a9-2159f4885b10","metadata":{"id":"dffa4f70-44d4-4be4-89a9-2159f4885b10"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/06_finetuning/figures/03.png?raw=1\" width=900px>"]},{"cell_type":"markdown","id":"dd79a74e-befb-491c-be49-f777a6a5b6a6","metadata":{"id":"dd79a74e-befb-491c-be49-f777a6a5b6a6"},"source":["- Suppose we use Alpaca-style prompt formatting, which was the original prompt template for instruction finetuning\n","- Shown below is how we format the input that we would pass as input to the LLM"]},{"cell_type":"code","execution_count":5,"id":"Jhk37nnJnkBh","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1720563124769,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"Jhk37nnJnkBh"},"outputs":[],"source":["def format_input(entry):\n","    instruction_text = (\n","        f\"Below is an instruction that describes a task. \"\n","        f\"Write a response that appropriately completes the request.\"\n","        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","    )\n","\n","    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","\n","    return instruction_text + input_text"]},{"cell_type":"markdown","id":"011e78b4-e89a-4653-a2ee-7b2739ca04d6","metadata":{"id":"011e78b4-e89a-4653-a2ee-7b2739ca04d6"},"source":["- A formatted response with input field looks like as shown below"]},{"cell_type":"code","execution_count":6,"id":"F9UQRfjzo4Js","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1720563128820,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"F9UQRfjzo4Js","outputId":"daab2ead-f7f6-4612-f57f-19fd39f269e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Identify the correct spelling of the following word.\n","\n","### Input:\n","Ocassion\n","\n","### Response:\n","The correct spelling is 'Occasion.'\n"]}],"source":["model_input = format_input(data[50])\n","desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n","\n","print(model_input + desired_response)"]},{"cell_type":"markdown","id":"4dc93ddf-431c-49c0-96f2-fb3a79c4d94c","metadata":{"id":"4dc93ddf-431c-49c0-96f2-fb3a79c4d94c"},"source":["- Below is a formatted response without an input field"]},{"cell_type":"code","execution_count":7,"id":"a3891fa9-f738-41cd-946c-80ef9a99c346","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168,"status":"ok","timestamp":1720563130453,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"a3891fa9-f738-41cd-946c-80ef9a99c346","outputId":"44b37b0c-ab88-445d-be27-4000a195a570"},"outputs":[{"name":"stdout","output_type":"stream","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","What is an antonym of 'complicated'?\n","\n","### Response:\n","An antonym of 'complicated' is 'simple'.\n"]}],"source":["model_input = format_input(data[999])\n","desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n","\n","print(model_input + desired_response)"]},{"cell_type":"markdown","id":"b9af423f-aad9-4b3c-bea5-153021c04862","metadata":{"id":"b9af423f-aad9-4b3c-bea5-153021c04862"},"source":["- Tokenized, this looks like as follows\n","\n","<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/06_finetuning/figures/04.png?raw=1\" width=1000px>"]},{"cell_type":"markdown","id":"9e5bd7bc-f347-4cf8-a0c2-94cb8799e427","metadata":{"id":"9e5bd7bc-f347-4cf8-a0c2-94cb8799e427"},"source":["- To make it work with batches, we add \"padding\" tokens"]},{"cell_type":"markdown","id":"65c4d943-4aa8-4a44-874e-05bc6831fbd3","metadata":{"id":"65c4d943-4aa8-4a44-874e-05bc6831fbd3"},"source":["- Tokenized, this looks like as follows\n","\n","<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/06_finetuning/figures/05.png?raw=1\" width=1000px>"]},{"cell_type":"markdown","id":"1ff6c2a5-449c-49e7-ba35-402e1cdbb2ef","metadata":{"id":"1ff6c2a5-449c-49e7-ba35-402e1cdbb2ef"},"source":["- Above, only the inputs are shown for simplicity; however, similar to pretraining, the target tokens are shifted by 1 position:"]},{"cell_type":"markdown","id":"0386b6fe-3455-4e70-becd-a5a4681ba2ef","metadata":{"id":"0386b6fe-3455-4e70-becd-a5a4681ba2ef"},"source":["- Tokenized, this looks like as follows\n","\n","<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/06_finetuning/figures/06.png?raw=1\" width=700px>"]},{"cell_type":"markdown","id":"cef09d21-b652-4760-abea-4f76920e6a25","metadata":{"id":"cef09d21-b652-4760-abea-4f76920e6a25"},"source":["- In addition, it is also common to mask the target text\n","- By default, PyTorch has the `cross_entropy(..., ignore_index=-100)` setting to ignore examples corresponding to the label -100\n","- Using this -100 `ignore_index`, we can ignore the additional end-of-text (padding) tokens in the batches that we used to pad the training examples to equal length\n","- However, we don't want to ignore the first instance of the end-of-text (padding) token (50256) because it can help signal to the LLM when the response is complete"]},{"cell_type":"markdown","id":"fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39","metadata":{"id":"fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39"},"source":["- Tokenized, this looks like as follows\n","\n","<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/06_finetuning/figures/07.png?raw=1\" width=1000px>"]},{"cell_type":"markdown","id":"c024bfa4-1a7a-4751-b5a1-827225a3478b","metadata":{"id":"c024bfa4-1a7a-4751-b5a1-827225a3478b"},"source":["**LLM Workshop 2024 by Sebastian Raschka**"]},{"cell_type":"markdown","id":"58b8c870-fb72-490e-8916-d8129bd5d1ff","metadata":{"id":"58b8c870-fb72-490e-8916-d8129bd5d1ff"},"source":["---\n","\n","# 6) Instruction finetuning (part 2; finetuning)"]},{"cell_type":"markdown","id":"013b3a3f-f300-4994-b704-624bdcd6371b","metadata":{"id":"013b3a3f-f300-4994-b704-624bdcd6371b"},"source":["- In this notebook, we get to the actual finetuning part\n","- But first, let's briefly introduce a technique, called LoRA, that makes the finetuning more efficient\n","- It's not required to use LoRA, but it can result in noticeable memory savings while still resulting in good modeling performance"]},{"cell_type":"markdown","id":"21532056-0ef4-4c98-82c7-e91f61c6485e","metadata":{"id":"21532056-0ef4-4c98-82c7-e91f61c6485e"},"source":["\n","# 6.1 Introduction to LoRA"]},{"cell_type":"markdown","id":"66edc999-3d91-4a1c-a157-9d056392e8d8","metadata":{"id":"66edc999-3d91-4a1c-a157-9d056392e8d8"},"source":["- Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters\n","- This approach is important because it allows for efficient finetuning of large models on task-specific data, significantly reducing the computational cost and time required for finetuning"]},{"cell_type":"markdown","id":"5bb75b5d-d59c-4948-821a-1594a5883dc1","metadata":{"id":"5bb75b5d-d59c-4948-821a-1594a5883dc1"},"source":["- Suppose we have a large weight matrix $W$ for a given layer\n","- During backpropagation, we learn a $\\Delta W$ matrix, which contains information on how much we want to update the original weights to minimize the loss function during training\n","- In regular training and finetuning, the weight update is defined as follows:\n","\n","$$W_{\\text{updated}} = W + \\Delta W$$\n","\n","- The LoRA method proposed by [Hu et al.](https://arxiv.org/abs/2106.09685) offers a more efficient alternative to computing the weight updates $\\Delta W$ by learning an approximation of it, $\\Delta W \\approx AB$.\n","- In other words, in LoRA, we have the following, where $A$ and $B$ are two small weight matrices:\n","\n","$$W_{\\text{updated}} = W + AB$$\n","\n","- The figure below illustrates these formulas for full finetuning and LoRA side by side"]},{"cell_type":"markdown","id":"a8a7419d-cae9-4525-bb44-1641f6ef4f3b","metadata":{"id":"a8a7419d-cae9-4525-bb44-1641f6ef4f3b"},"source":["<img src=\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/figures/08.png\" width=\"1100px\">"]},{"cell_type":"markdown","id":"4edd43c9-8ec5-48e6-b3fc-5fb3c16037cc","metadata":{"id":"4edd43c9-8ec5-48e6-b3fc-5fb3c16037cc"},"source":["- If you paid close attention, the full finetuning and LoRA depictions in the figure above look slightly different from the formulas I have shown earlier\n","- That's due to the distributive law of matrix multiplication: we don't have to add the weights with the updated weights but can keep them separate\n","- For instance, if $x$ is the input data, then we can write the following for regular finetuning:\n","\n","$$x (W+\\Delta W) = x W + x \\Delta W$$\n","\n","- Similarly, we can write the following for LoRA:\n","\n","$$x (W+A B) = x W + x A B$$\n","\n","- The fact that we can keep the LoRA weight matrices separate makes LoRA especially attractive\n","- In practice, this means that we don't have to modify the weights of the pretrained model at all, as we can apply the LoRA matrices on the fly\n","- After setting up the dataset and loading the model, we will implement LoRA in the code to make these concepts less abstract"]},{"cell_type":"markdown","id":"b214882e-68d5-46e4-a2a0-bae3e8500a9e","metadata":{"id":"b214882e-68d5-46e4-a2a0-bae3e8500a9e"},"source":["<img src=\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/figures/09.png\" width=\"800px\">"]},{"cell_type":"markdown","id":"78e4d859-440f-4438-9cea-54311a5fc13a","metadata":{"id":"78e4d859-440f-4438-9cea-54311a5fc13a"},"source":["\n","\n","# 6.2 Creating training and test sets"]},{"cell_type":"markdown","id":"10333f8b-64dc-40f7-9508-657183384231","metadata":{"id":"10333f8b-64dc-40f7-9508-657183384231"},"source":["- There's one more thing before we can start finetuning: creating the training and test subsets\n","- We will use 85% of the data for training and the remaining 15% for testing"]},{"cell_type":"code","execution_count":2,"id":"6cabbaac-c690-448a-9cfb-33418fcfc27b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":207,"status":"ok","timestamp":1720563621524,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"6cabbaac-c690-448a-9cfb-33418fcfc27b","outputId":"b65f5cdf-ae12-4185-8369-557d2d31fced"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of entries: 1100\n"]}],"source":["import json\n","\n","\n","file_path = \"instruction-data.json\"\n","\n","with open(file_path, \"r\") as file:\n","    data = json.load(file)\n","print(\"Number of entries:\", len(data))"]},{"cell_type":"code","execution_count":3,"id":"293281ab-5c7e-444c-bc13-b63a65be68f1","metadata":{"executionInfo":{"elapsed":220,"status":"ok","timestamp":1720563623635,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"293281ab-5c7e-444c-bc13-b63a65be68f1"},"outputs":[],"source":["train_portion = int(len(data) * 0.85)  # 85% for training\n","test_portion = int(len(data) * 0.15)    # 15% for testing\n","\n","train_data = data[:train_portion]\n","test_data = data[train_portion:]"]},{"cell_type":"code","execution_count":4,"id":"7e1027bb-0b3f-4e5f-863c-be5d91ef49ea","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200,"status":"ok","timestamp":1720563627290,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"7e1027bb-0b3f-4e5f-863c-be5d91ef49ea","outputId":"cedece31-6ca8-41c3-95e5-1cecfb95288b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set length: 935\n","Test set length: 165\n"]}],"source":["print(\"Training set length:\", len(train_data))\n","print(\"Test set length:\", len(test_data))"]},{"cell_type":"code","execution_count":5,"id":"ad75b683-856a-425c-ba8f-c887a937baa0","metadata":{"executionInfo":{"elapsed":171,"status":"ok","timestamp":1720563628885,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"ad75b683-856a-425c-ba8f-c887a937baa0"},"outputs":[],"source":["with open(\"train.json\", \"w\") as json_file:\n","    json.dump(train_data, json_file, indent=4)\n","\n","with open(\"test.json\", \"w\") as json_file:\n","    json.dump(test_data, json_file, indent=4)"]},{"cell_type":"markdown","id":"46318684-d607-4f40-aa8a-cd2b17f879c6","metadata":{"id":"46318684-d607-4f40-aa8a-cd2b17f879c6"},"source":["\n","# 6.3 Instruction finetuning"]},{"cell_type":"markdown","id":"95092e6d-cfe0-45e1-a8d6-aa5f72adaa32","metadata":{"id":"95092e6d-cfe0-45e1-a8d6-aa5f72adaa32"},"source":["- Using LitGPT, we can finetune the model via `litgpt finetune model_dir`\n","- However, here, we will use LoRA finetuning `litgpt finetune_lora model_dir` since it will be quicker and less resource intensive"]},{"cell_type":"code","execution_count":6,"id":"H9Y9ehsVlfAp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8657,"status":"ok","timestamp":1720563843827,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"H9Y9ehsVlfAp","outputId":"0da98df8-b941-4e35-afb4-bd1275705aa1"},"outputs":[{"name":"stdout","output_type":"stream","text":["repo_id: list\n","Please specify --repo_id <repo_id>. Available values:\n","codellama/CodeLlama-13b-hf\n","codellama/CodeLlama-13b-Instruct-hf\n","codellama/CodeLlama-13b-Python-hf\n","codellama/CodeLlama-34b-hf\n","codellama/CodeLlama-34b-Instruct-hf\n","codellama/CodeLlama-34b-Python-hf\n","codellama/CodeLlama-70b-hf\n","codellama/CodeLlama-70b-Instruct-hf\n","codellama/CodeLlama-70b-Python-hf\n","codellama/CodeLlama-7b-hf\n","codellama/CodeLlama-7b-Instruct-hf\n","codellama/CodeLlama-7b-Python-hf\n","databricks/dolly-v2-12b\n","databricks/dolly-v2-3b\n","databricks/dolly-v2-7b\n","EleutherAI/pythia-1.4b\n","EleutherAI/pythia-1.4b-deduped\n","EleutherAI/pythia-12b\n","EleutherAI/pythia-12b-deduped\n","EleutherAI/pythia-14m\n","EleutherAI/pythia-160m\n","EleutherAI/pythia-160m-deduped\n","EleutherAI/pythia-1b\n","EleutherAI/pythia-1b-deduped\n","EleutherAI/pythia-2.8b\n","EleutherAI/pythia-2.8b-deduped\n","EleutherAI/pythia-31m\n","EleutherAI/pythia-410m\n","EleutherAI/pythia-410m-deduped\n","EleutherAI/pythia-6.9b\n","EleutherAI/pythia-6.9b-deduped\n","EleutherAI/pythia-70m\n","EleutherAI/pythia-70m-deduped\n","garage-bAInd/Camel-Platypus2-13B\n","garage-bAInd/Camel-Platypus2-70B\n","garage-bAInd/Platypus-30B\n","garage-bAInd/Platypus2-13B\n","garage-bAInd/Platypus2-70B\n","garage-bAInd/Platypus2-70B-instruct\n","garage-bAInd/Platypus2-7B\n","garage-bAInd/Stable-Platypus2-13B\n","google/codegemma-7b-it\n","google/gemma-2b\n","google/gemma-2b-it\n","google/gemma-7b\n","google/gemma-7b-it\n","h2oai/h2o-danube2-1.8b-chat\n","keeeeenw/MicroLlama\n","lmsys/longchat-13b-16k\n","lmsys/longchat-7b-16k\n","lmsys/vicuna-13b-v1.3\n","lmsys/vicuna-13b-v1.5\n","lmsys/vicuna-13b-v1.5-16k\n","lmsys/vicuna-33b-v1.3\n","lmsys/vicuna-7b-v1.3\n","lmsys/vicuna-7b-v1.5\n","lmsys/vicuna-7b-v1.5-16k\n","meta-llama/Llama-2-13b-chat-hf\n","meta-llama/Llama-2-13b-hf\n","meta-llama/Llama-2-70b-chat-hf\n","meta-llama/Llama-2-70b-hf\n","meta-llama/Llama-2-7b-chat-hf\n","meta-llama/Llama-2-7b-hf\n","meta-llama/Meta-Llama-3-70B\n","meta-llama/Meta-Llama-3-70B-Instruct\n","meta-llama/Meta-Llama-3-8B\n","meta-llama/Meta-Llama-3-8B-Instruct\n","microsoft/phi-1_5\n","microsoft/phi-2\n","microsoft/Phi-3-mini-4k-instruct\n","mistralai/Mistral-7B-Instruct-v0.1\n","mistralai/Mistral-7B-Instruct-v0.2\n","mistralai/Mistral-7B-Instruct-v0.3\n","mistralai/Mistral-7B-v0.1\n","mistralai/Mistral-7B-v0.3\n","mistralai/Mixtral-8x7B-Instruct-v0.1\n","mistralai/Mixtral-8x7B-v0.1\n","NousResearch/Nous-Hermes-13b\n","NousResearch/Nous-Hermes-llama-2-7b\n","NousResearch/Nous-Hermes-Llama2-13b\n","openlm-research/open_llama_13b\n","openlm-research/open_llama_3b\n","openlm-research/open_llama_7b\n","stabilityai/FreeWilly2\n","stabilityai/stable-code-3b\n","stabilityai/stablecode-completion-alpha-3b\n","stabilityai/stablecode-completion-alpha-3b-4k\n","stabilityai/stablecode-instruct-alpha-3b\n","stabilityai/stablelm-3b-4e1t\n","stabilityai/stablelm-base-alpha-3b\n","stabilityai/stablelm-base-alpha-7b\n","stabilityai/stablelm-tuned-alpha-3b\n","stabilityai/stablelm-tuned-alpha-7b\n","stabilityai/stablelm-zephyr-3b\n","tiiuae/falcon-180B\n","tiiuae/falcon-180B-chat\n","tiiuae/falcon-40b\n","tiiuae/falcon-40b-instruct\n","tiiuae/falcon-7b\n","tiiuae/falcon-7b-instruct\n","TinyLlama/TinyLlama-1.1B-Chat-v1.0\n","TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n","togethercomputer/LLaMA-2-7B-32K\n","togethercomputer/RedPajama-INCITE-7B-Base\n","togethercomputer/RedPajama-INCITE-7B-Chat\n","togethercomputer/RedPajama-INCITE-7B-Instruct\n","togethercomputer/RedPajama-INCITE-Base-3B-v1\n","togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n","togethercomputer/RedPajama-INCITE-Chat-3B-v1\n","togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n","togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n","togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n","Trelis/Llama-2-7b-chat-hf-function-calling-v2\n","unsloth/Mistral-7B-v0.2\n"]}],"source":["!litgpt download list"]},{"cell_type":"code","execution_count":24,"id":"43e08d63","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["repo_id: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n","Setting HF_HUB_ENABLE_HF_TRANSFER=1\n","/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n","For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n","  warnings.warn(\n","config.json: 100%|█████████████████████████████| 560/560 [00:00<00:00, 7.65MB/s]\n","generation_config.json: 100%|██████████████████| 129/129 [00:00<00:00, 1.17MB/s]\n","pytorch_model.bin: 100%|████████████████████| 4.40G/4.40G [00:24<00:00, 181MB/s]\n","tokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:00<00:00, 62.1MB/s]\n","tokenizer.model: 100%|████████████████████████| 500k/500k [00:00<00:00, 261MB/s]\n","tokenizer_config.json: 100%|███████████████████| 776/776 [00:00<00:00, 12.1MB/s]\n","Converting checkpoint files to LitGPT format.\n","{'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\n"," 'debug_mode': False,\n"," 'dtype': None,\n"," 'model_name': None}\n","Loading weights: pytorch_model.bin: 100%|███████████████| 00:06<00:00, 14.56it/s\n","Saving converted checkpoint to checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n"]}],"source":["!litgpt download TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"]},{"cell_type":"code","execution_count":37,"id":"415041a4-5533-49ed-bcc8-3e1cb249bc65","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1056920,"status":"ok","timestamp":1720565043611,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"415041a4-5533-49ed-bcc8-3e1cb249bc65","outputId":"498a3289-2654-44d8-f49f-bdb15bb590e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\n"," 'data': JSON(json_path=PosixPath('train.json'),\n","              mask_prompt=False,\n","              val_split_fraction=0.1,\n","              prompt_style=<litgpt.prompts.Alpaca object at 0x7fba8ee8a6b0>,\n","              ignore_index=-100,\n","              seed=42,\n","              num_workers=4),\n"," 'devices': 1,\n"," 'eval': EvalArgs(interval=100,\n","                  max_new_tokens=100,\n","                  max_iters=100,\n","                  initial_validation=False,\n","                  final_validation=True),\n"," 'logger_name': 'csv',\n"," 'lora_alpha': 16,\n"," 'lora_dropout': 0.05,\n"," 'lora_head': False,\n"," 'lora_key': False,\n"," 'lora_mlp': False,\n"," 'lora_projection': False,\n"," 'lora_query': True,\n"," 'lora_r': 8,\n"," 'lora_value': True,\n"," 'optimizer': 'AdamW',\n"," 'out_dir': PosixPath('out/finetune/lora'),\n"," 'precision': None,\n"," 'quantize': None,\n"," 'seed': 1337,\n"," 'train': TrainArgs(save_interval=1000,\n","                    log_interval=100,\n","                    global_batch_size=16,\n","                    micro_batch_size=1,\n","                    lr_warmup_steps=100,\n","                    lr_warmup_fraction=None,\n","                    epochs=3,\n","                    max_tokens=None,\n","                    max_steps=None,\n","                    max_seq_length=None,\n","                    tie_embeddings=None,\n","                    max_norm=None,\n","                    min_lr=6e-05)}\n","Using bfloat16 Automatic Mixed Precision (AMP)\n","Seed set to 1337\n","Number of trainable parameters: 1,126,400\n","Number of non-trainable parameters: 1,100,048,384\n","The longest sequence length in the train data is 117, the model's maximum sequence length is 117 and context length is 2048\n","Verifying settings ...\n","Missing logger folder: /teamspace/studios/this_studio/out/finetune/lora/logs/csv\n","Epoch 1 | iter 100 step 6 | loss train: 2.272, val: n/a | iter time: 63.30 ms\n","Epoch 1 | iter 200 step 12 | loss train: 2.258, val: n/a | iter time: 65.24 ms\n","Epoch 1 | iter 300 step 18 | loss train: 1.998, val: n/a | iter time: 62.67 ms\n","Epoch 1 | iter 400 step 25 | loss train: 1.422, val: n/a | iter time: 64.71 ms (step)\n","Epoch 1 | iter 500 step 31 | loss train: 0.932, val: n/a | iter time: 64.65 ms\n","Epoch 1 | iter 600 step 37 | loss train: 0.585, val: n/a | iter time: 63.30 ms\n","Epoch 1 | iter 700 step 43 | loss train: 0.577, val: n/a | iter time: 61.71 ms\n","Epoch 1 | iter 800 step 50 | loss train: 0.535, val: n/a | iter time: 62.66 ms (step)\n","Epoch 2 | iter 900 step 56 | loss train: 0.442, val: n/a | iter time: 62.37 ms\n","Epoch 2 | iter 1000 step 62 | loss train: 0.536, val: n/a | iter time: 62.04 ms\n","Epoch 2 | iter 1100 step 68 | loss train: 0.476, val: n/a | iter time: 61.98 ms\n","Epoch 2 | iter 1200 step 75 | loss train: 0.492, val: n/a | iter time: 64.18 ms (step)\n","Epoch 2 | iter 1300 step 81 | loss train: 0.412, val: n/a | iter time: 61.56 ms\n","Epoch 2 | iter 1400 step 87 | loss train: 0.417, val: n/a | iter time: 64.03 ms\n","Epoch 2 | iter 1500 step 93 | loss train: 0.408, val: n/a | iter time: 61.48 ms\n","/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Epoch 2 | iter 1600 step 100 | loss train: 0.409, val: n/a | iter time: 64.74 ms (step)\n","Validating ...\n","Recommend a movie for me to watch during the weekend and explain the reason.\n","Length of encoded instruction (54) and eval.max_new_tokens (100) exceeds model.max_seq_length (117) used for training. Skipping example generation for efficiency. The model's supported context size (post-training) is 2048.\n","iter 1600: val loss 0.4754, val time: 3437.25 ms\n","Epoch 3 | iter 1700 step 106 | loss train: 0.375, val: 0.475 | iter time: 61.53 ms\n","Epoch 3 | iter 1800 step 112 | loss train: 0.423, val: 0.475 | iter time: 63.92 ms\n","Epoch 3 | iter 1900 step 118 | loss train: 0.352, val: 0.475 | iter time: 78.35 ms\n","Epoch 3 | iter 2000 step 125 | loss train: 0.324, val: 0.475 | iter time: 61.43 ms (step)\n","Epoch 3 | iter 2100 step 131 | loss train: 0.429, val: 0.475 | iter time: 60.44 ms\n","Epoch 3 | iter 2200 step 137 | loss train: 0.469, val: 0.475 | iter time: 60.75 ms\n","Epoch 3 | iter 2300 step 143 | loss train: 0.408, val: 0.475 | iter time: 59.78 ms\n","Epoch 3 | iter 2400 step 150 | loss train: 0.427, val: 0.475 | iter time: 61.56 ms (step)\n","Epoch 3 | iter 2500 step 156 | loss train: 0.404, val: 0.475 | iter time: 60.39 ms\n","Training time: 161.93s\n","Memory used: 6.77 GB\n","Validating ...\n","Final evaluation | val loss: 0.447 | val ppl: 1.564\n","Saving LoRA weights to '/teamspace/studios/this_studio/out/finetune/lora/final/lit_model.pth.lora'\n","{'checkpoint_dir': PosixPath('/teamspace/studios/this_studio/out/finetune/lora/final'),\n"," 'precision': None,\n"," 'pretrained_checkpoint_dir': None}\n","Saved merged weights to '/teamspace/studios/this_studio/out/finetune/lora/final/lit_model.pth'\n"]}],"source":["!litgpt finetune_lora TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T \\\n","--data JSON \\\n","--data.val_split_fraction 0.1 \\\n","--data.json_path train.json \\\n","--train.epochs 3 \\\n","--train.log_interval 100"]},{"cell_type":"markdown","id":"c4edc913","metadata":{"id":"c4edc913"},"source":["\n","# Exercise 1: Generate and save the test set model responses of the base model"]},{"cell_type":"markdown","id":"c0324437-6d2f-4f59-9e78-39dd4d890a98","metadata":{"id":"c0324437-6d2f-4f59-9e78-39dd4d890a98"},"source":["- In this excercise, we are collecting the model responses on the test dataset so that we can evaluate them later\n","\n","\n","- Starting with the original model before finetuning, load the model using the LitGPT Python API (`LLM.load` ...)\n","- Then use the `LLM.generate` function to generate the responses for the test data\n","- The following utility function will help you to format the test set entries as input text for the LLM"]},{"cell_type":"code","execution_count":38,"id":"af804ccf-2fd5-4d16-ae9a-15274f405b60","metadata":{"id":"af804ccf-2fd5-4d16-ae9a-15274f405b60"},"outputs":[{"name":"stdout","output_type":"stream","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Rewrite the sentence using a simile.\n","\n","### Input:\n","The car is very fast.\n"]}],"source":["def format_input(entry):\n","    instruction_text = (\n","        f\"Below is an instruction that describes a task. \"\n","        f\"Write a response that appropriately completes the request.\"\n","        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","    )\n","\n","    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","\n","    return instruction_text + input_text\n","\n","print(format_input(test_data[0]))"]},{"cell_type":"code","execution_count":42,"id":"5b733e6f","metadata":{},"outputs":[{"data":{"text/plain":["{'instruction': 'What type of cloud is typically associated with thunderstorms?',\n"," 'input': '',\n"," 'output': 'The type of cloud typically associated with thunderstorms is cumulonimbus.'}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["test_data[1]"]},{"cell_type":"code","execution_count":14,"id":"411dbd8d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","What type of cloud is typically associated with thunderstorms?\n"]}],"source":["print(format_input(test_data[1]))"]},{"cell_type":"markdown","id":"b6cc7b91-6c2e-4a09-98b6-0e3cd1192aec","metadata":{"id":"b6cc7b91-6c2e-4a09-98b6-0e3cd1192aec"},"source":["- Using this utility function, generate and save all the test set responses generated by the model and add them to the `test_set`\n","- For example, if `test_data[0]` entry is as follows before:\n","    \n","```\n","{'instruction': 'Rewrite the sentence using a simile.',\n"," 'input': 'The car is very fast.',\n"," 'output': 'The car is as fast as lightning.'}\n","```\n","\n","- Modify the `test_data` entry so that it contains the model response:\n","    \n","```\n","{'instruction': 'Rewrite the sentence using a simile.',\n"," 'input': 'The car is very fast.',\n"," 'output': 'The car is as fast as lightning.',\n"," 'base_model': 'The car is as fast as a cheetah sprinting across the savannah.'\n","}\n","```\n","\n","- Do this for all test set entries, and then save the modified `test_data` dictionary as `test_base_model.json`\n"]},{"cell_type":"code","execution_count":43,"id":"cf2c2ba0","metadata":{"id":"cf2c2ba0"},"outputs":[],"source":["from litgpt import LLM\n","\n","llm = LLM.load(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")"]},{"cell_type":"code","execution_count":44,"id":"c3b43aec-a9c4-45ee-83a3-146b2413417e","metadata":{"id":"c3b43aec-a9c4-45ee-83a3-146b2413417e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 165/165 [01:57<00:00,  1.41it/s]\n"]}],"source":["from tqdm import tqdm\n","\n","for i in tqdm(range(len(test_data))):\n","    response = llm.generate(format_input(test_data[i]))\n","    test_data[i][\"base_model\"] = response\n","\n","with open(\"test_base_model.json\", mode=\"wt\", encoding=\"utf-8\") as f:\n","    json.dump(test_data, f)"]},{"cell_type":"code","execution_count":45,"id":"88b92e98","metadata":{"id":"88b92e98"},"outputs":[{"data":{"text/plain":["{'instruction': 'What type of cloud is typically associated with thunderstorms?',\n"," 'input': '',\n"," 'output': 'The type of cloud typically associated with thunderstorms is cumulonimbus.',\n"," 'base_model': '\\n\\n### Instruction:\\nWhich provides power and efficiency for a huge amount of devices? \\n\\n### Instruction:\\nWhat is the most common way people use solar power?\\n\\n### Answer:\\nThe website'}"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["test_data[1]"]},{"cell_type":"markdown","id":"5c45c1d2-3295-434f-b30c-3cd4cac89367","metadata":{"id":"5c45c1d2-3295-434f-b30c-3cd4cac89367"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# Exercise 2: Generate and save the test set model responses of the finetuned model"]},{"cell_type":"code","execution_count":46,"id":"ri74lvurqzRm","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68536,"status":"ok","timestamp":1720565148982,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"ri74lvurqzRm","outputId":"7197a5b2-ca0c-4d63-cdb5-a21b823be839"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\n"," 'compile': False,\n"," 'max_new_tokens': 256,\n"," 'num_samples': 1,\n"," 'precision': None,\n"," 'prompt': 'What food do llamas eat?',\n"," 'quantize': None,\n"," 'temperature': 0.8,\n"," 'top_k': 50,\n"," 'top_p': 1.0}\n","Loading model 'checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T/lit_model.pth' with {'name': 'tiny-llama-1.1b', 'hf_config': {'name': 'TinyLlama-1.1B-intermediate-step-1431k-3T', 'org': 'TinyLlama'}, 'scale_embeddings': False, 'block_size': 2048, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 22, 'n_head': 32, 'head_size': 64, 'n_embd': 2048, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 4, 'shared_attention_norm': False, 'norm_class_name': 'RMSNorm', 'norm_eps': 1e-05, 'mlp_class_name': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 5632, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'rope_n_elem': 64}\n","Time to instantiate model: 0.08 seconds.\n","Time to load the model weights: 2.66 seconds.\n","Seed set to 1234\n","What food do llamas eat? What is the main food of a llama? What food do llamas eat?\n","What is the main food of a llama?\n","The main food of a llama is plants.\n","What is it that llamas eat?\n","llamas eat grass when they are young, then as they grow, they eat plants and other foods such as seeds.\n","What does a llama eat?\n","a llama eat grass and grain.\n","What do llamas eat?\n","Llamas eat grass (or other plants) when young, then as they grow they eat plant, seeds, and other foods such as grains.\n","What do llamas eat?\n","Can a llama eat wheat?\n","Llamas eat grass, as do all other domestic animals. Wheat is not a grain.\n","What does a llama eat?\n","what does a llama eat?\n","What do llamas eat?\n","Llamas eat plants, grass, other grains, and manure.\n","What do llamas eat?\n","What do llamas eat?\n","Llamas eat meat. They eat grass when they are young, but as they grow, they eat other foods such as grains\n","Time for inference 1: 4.08 sec total, 62.72 tokens/sec\n","Memory used: 2.28 GB\n"]}],"source":["!litgpt generate TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T \\\n","  --max_new_tokens 256"]},{"cell_type":"markdown","id":"d226d857-68f7-43f3-90c8-4534747b158a","metadata":{"id":"d226d857-68f7-43f3-90c8-4534747b158a"},"source":["- Repeat the steps from the previous exercise but this time collect the responses of the finetuned model\n","- Save the resulting `test_data` dictionary as `test_base_and_finetuned_model.json`"]},{"cell_type":"code","execution_count":47,"id":"09912041","metadata":{},"outputs":[],"source":["from litgpt import LLM\n","\n","# Save memory by removing the previous model\n","# del llm\n","\n","llm_lora = LLM.load(\"/teamspace/studios/this_studio/out/finetune/lora/final\")"]},{"cell_type":"code","execution_count":48,"id":"614d125f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 165/165 [00:39<00:00,  4.13it/s]\n"]}],"source":["from tqdm import tqdm\n","\n","for i in tqdm(range(len(test_data))):\n","    response = llm_lora.generate(format_input(test_data[i]))\n","    test_data[i][\"finetuned_model\"] = response\n","\n","with open(\"test_base_and_finetuned_model.json\", mode=\"wt\", encoding=\"utf-8\") as f:\n","    json.dump(test_data, f)"]},{"cell_type":"code","execution_count":49,"id":"279d1464","metadata":{},"outputs":[{"data":{"text/plain":["{'instruction': 'What type of cloud is typically associated with thunderstorms?',\n"," 'input': '',\n"," 'output': 'The type of cloud typically associated with thunderstorms is cumulonimbus.',\n"," 'base_model': '\\n\\n### Instruction:\\nWhich provides power and efficiency for a huge amount of devices? \\n\\n### Instruction:\\nWhat is the most common way people use solar power?\\n\\n### Answer:\\nThe website',\n"," 'finetuned_model': 'Clouds that form when rain and thunderstorms combine are called wet-lift clouds.'}"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["test_data[1]"]},{"cell_type":"markdown","id":"YBGpru2yhUoO","metadata":{"id":"YBGpru2yhUoO"},"source":["**LLM Workshop 2024 by Sebastian Raschka**"]},{"cell_type":"markdown","id":"MxZHkj7dhUob","metadata":{"id":"MxZHkj7dhUob"},"source":["---\n","\n","# 6) Instruction finetuning (part 3; benchmark evaluation)"]},{"cell_type":"markdown","id":"xxDedTRshUoe","metadata":{"id":"xxDedTRshUoe"},"source":["- In the previous notebook, we finetuned the LLM; in this notebook, we evaluate it using popular benchmark methods\n","\n","- There are 3 main types of model evaluation\n","\n","  1. MMLU-style Q&A\n","  2. LLM-based automatic scoring\n","  3. Human ratings by relative preference\n","  \n","  \n"]},{"cell_type":"markdown","id":"dfc0f1d9-f72b-4cf7-aef6-0407acf1a46e","metadata":{"id":"dfc0f1d9-f72b-4cf7-aef6-0407acf1a46e"},"source":["<img src=\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/figures/10.png\" width=800px>\n","\n","<img src=\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/figures/11.png\" width=800px>"]},{"cell_type":"markdown","id":"c389e66f-5463-4e40-820c-55d1e3eb5077","metadata":{"id":"c389e66f-5463-4e40-820c-55d1e3eb5077"},"source":["\n","<br>\n","<br>\n","<br>\n","\n","\n","<img src=\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/figures/13.png\" width=800px>\n","\n"]},{"cell_type":"markdown","id":"0070e756","metadata":{"id":"0070e756"},"source":["<img src=\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/figures/14.png\" width=800px>\n","\n"]},{"cell_type":"markdown","id":"45464478-d7b1-4300-b251-a6e125c8ae52","metadata":{"id":"45464478-d7b1-4300-b251-a6e125c8ae52"},"source":["## https://tatsu-lab.github.io/alpaca_eval/"]},{"cell_type":"markdown","id":"de7ba804","metadata":{"id":"de7ba804"},"source":["<img src=\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/06_finetuning/figures/15.png\" width=800px>\n","\n","## https://chat.lmsys.org"]},{"cell_type":"markdown","id":"e084ce8a-bcba-4c67-ad49-0a645ce65eb7","metadata":{"id":"e084ce8a-bcba-4c67-ad49-0a645ce65eb7","tags":[]},"source":["# 6.2 Evaluation"]},{"cell_type":"markdown","id":"7d0f6422-582f-4996-ae2a-dbacc9ebf86d","metadata":{"id":"7d0f6422-582f-4996-ae2a-dbacc9ebf86d"},"source":["- In this notebook, we do an MMLU-style evaluation in LitGPT, which is based on the [EleutherAI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)\n","- There are hundreds if not thousands of benchmarks; using the command below, we filter for MMLU subsets, because running the evaluation on the whole MMLU dataset would take a very long time"]},{"cell_type":"markdown","id":"67bb4cb5-e8ea-445b-8862-e7151c1544fc","metadata":{"id":"67bb4cb5-e8ea-445b-8862-e7151c1544fc"},"source":["- Let's say we are intrested in the `mmlu_philosophy` subset, we can evaluate the LLM on MMLU as follows"]},{"cell_type":"markdown","id":"IRKOmjh8hUo1","metadata":{"id":"IRKOmjh8hUo1"},"source":["# Exercise 3: Evaluate the finetuned LLM"]},{"cell_type":"code","execution_count":53,"id":"f7b6218c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'batch_size': 4,\n"," 'checkpoint_dir': PosixPath('out/finetune/lora/final'),\n"," 'device': None,\n"," 'dtype': None,\n"," 'force_conversion': False,\n"," 'limit': None,\n"," 'num_fewshot': None,\n"," 'out_dir': PosixPath('eval_finetuned'),\n"," 'save_filepath': None,\n"," 'seed': 1234,\n"," 'tasks': 'mmlu_philosophy'}\n","2024-09-14 05:15:50.484328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-09-14 05:15:50.500770: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-09-14 05:15:50.500808: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-09-14 05:15:50.511345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-09-14 05:15:51.278887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2024-09-14:05:15:52,499 INFO     [huggingface.py:170] Using device 'cuda'\n","2024-09-14:05:15:56,478 INFO     [evaluator.py:152] Setting random seed to 1234 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n","2024-09-14:05:15:56,478 INFO     [evaluator.py:203] Using pre-initialized model\n","2024-09-14:05:16:01,370 INFO     [evaluator.py:261] Setting fewshot random generator seed to 1234\n","2024-09-14:05:16:01,371 INFO     [task.py:411] Building contexts for mmlu_philosophy on rank 0...\n","100%|████████████████████████████████████████| 311/311 [00:00<00:00, 916.83it/s]\n","2024-09-14:05:16:01,719 INFO     [evaluator.py:438] Running loglikelihood requests\n","Running loglikelihood requests:   0%|                  | 0/1244 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","Running loglikelihood requests: 100%|██████| 1244/1244 [00:03<00:00, 335.26it/s]\n","2024-09-14:05:16:05,781 WARNING  [huggingface.py:1315] Failed to get model SHA for /teamspace/studios/this_studio/eval_finetuned at revision main. Error: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/teamspace/studios/this_studio/eval_finetuned'. Use `repo_type` argument if needed.\n","fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n","Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","|  Tasks   |Version|Filter|n-shot|Metric|   |Value|   |Stderr|\n","|----------|------:|------|-----:|------|---|----:|---|-----:|\n","|philosophy|      0|none  |     0|acc   |↑  |0.283|±  |0.0256|\n","\n"]}],"source":["!litgpt evaluate out/finetune/lora/final \\\n","    --batch_size 4 \\\n","    --tasks \"mmlu_philosophy\" \\\n","    --out_dir \"eval_finetuned\""]},{"cell_type":"markdown","id":"136a4efe-fb99-4311-8679-e0a5b6282755","metadata":{"id":"136a4efe-fb99-4311-8679-e0a5b6282755"},"source":["**LLM Workshop 2024 by Sebastian Raschka**\n"]},{"cell_type":"markdown","id":"b1910a06-e8a3-40ac-8201-ff70615b1ba4","metadata":{"id":"b1910a06-e8a3-40ac-8201-ff70615b1ba4","tags":[]},"source":["---\n","\n","# 6) Instruction finetuning (part 4; evaluating instruction responses locally using a Llama 3 model)"]},{"cell_type":"markdown","id":"a128651b-f326-4232-a994-42f38b7ed520","metadata":{"id":"a128651b-f326-4232-a994-42f38b7ed520"},"source":["- This notebook uses an 8 billion parameter Llama 3 model through LitGPT to evaluate responses of instruction finetuned LLMs based on a dataset in JSON format that includes the generated model responses, for example:\n","\n","\n","\n","```python\n","{\n","    \"instruction\": \"What is the atomic number of helium?\",\n","    \"input\": \"\",\n","    \"output\": \"The atomic number of helium is 2.\",               # <-- The target given in the test set\n","    \"base_model\": \"\\nThe atomic number of helium is 3.0\", # <-- Response by an LLM\n","    \"finetuned_model\": \"\\nThe atomic number of helium is 2.\"    # <-- Response by a 2nd LLM\n","},\n","```\n","\n","- The code doesn't require a GPU and runs on a laptop (it was tested on a M3 MacBook Air)"]},{"cell_type":"code","execution_count":54,"id":"63610acc-db94-437f-8d38-e99dca0299cb","metadata":{"id":"63610acc-db94-437f-8d38-e99dca0299cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["tqdm version: 4.66.4\n"]}],"source":["from importlib.metadata import version\n","\n","pkgs = [\n","    \"tqdm\",    # Progress bar\n","]\n","\n","for p in pkgs:\n","    print(f\"{p} version: {version(p)}\")"]},{"cell_type":"markdown","id":"162a4739-6f03-4092-a5c2-f57a0b6a4c4d","metadata":{"id":"162a4739-6f03-4092-a5c2-f57a0b6a4c4d"},"source":["## 6.1 Load JSON Entries"]},{"cell_type":"markdown","id":"ca011a8b-20c5-4101-979e-9b5fccf62f8a","metadata":{"id":"ca011a8b-20c5-4101-979e-9b5fccf62f8a"},"source":["- Now, let's get to the data evaluation part\n","- Here, we assume that we saved the test dataset and the model responses as a JSON file that we can load as follows:"]},{"cell_type":"code","execution_count":1,"id":"8b2d393a-aa92-4190-9d44-44326a6f699b","metadata":{"id":"8b2d393a-aa92-4190-9d44-44326a6f699b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of entries: 165\n"]}],"source":["import json\n","\n","json_file = \"test_base_and_finetuned_model.json\"\n","\n","with open(json_file, \"r\") as file:\n","    json_data = json.load(file)\n","\n","print(\"Number of entries:\", len(json_data))"]},{"cell_type":"markdown","id":"b6c9751b-59b7-43fe-acc7-14e8daf2fa66","metadata":{"id":"b6c9751b-59b7-43fe-acc7-14e8daf2fa66"},"source":["- The structure of this file is as follows, where we have the given response in the test dataset (`'output'`) and responses by two different models (`'base_model'` and `'finetuned_model'`):"]},{"cell_type":"code","execution_count":2,"id":"7222fdc0-5684-4f2b-b741-3e341851359e","metadata":{"id":"7222fdc0-5684-4f2b-b741-3e341851359e"},"outputs":[{"data":{"text/plain":["{'instruction': 'What type of cloud is typically associated with thunderstorms?',\n"," 'input': '',\n"," 'output': 'The type of cloud typically associated with thunderstorms is cumulonimbus.',\n"," 'base_model': '\\n\\n### Instruction:\\nWhich provides power and efficiency for a huge amount of devices? \\n\\n### Instruction:\\nWhat is the most common way people use solar power?\\n\\n### Answer:\\nThe website',\n"," 'finetuned_model': 'Clouds that form when rain and thunderstorms combine are called wet-lift clouds.'}"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["json_data[1]"]},{"cell_type":"markdown","id":"fcf0331b-6024-4bba-89a9-a088b14a1046","metadata":{"id":"fcf0331b-6024-4bba-89a9-a088b14a1046"},"source":["- Below is a small utility function that formats the input for visualization purposes later:"]},{"cell_type":"code","execution_count":3,"id":"43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c","metadata":{"id":"43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Rewrite the sentence using a simile.\n","\n","### Input:\n","The car is very fast.\n"]}],"source":["def format_input(entry):\n","    instruction_text = (\n","        f\"Below is an instruction that describes a task. Write a response that \"\n","        f\"appropriately completes the request.\"\n","        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","    )\n","\n","    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","    instruction_text + input_text\n","\n","    return instruction_text + input_text\n","\n","print(format_input(json_data[0])) # input"]},{"cell_type":"code","execution_count":4,"id":"be927741","metadata":{"id":"be927741"},"outputs":[{"data":{"text/plain":["'The car is as fast as lightning.'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["json_data[0][\"output\"]"]},{"cell_type":"code","execution_count":5,"id":"d5537627","metadata":{"id":"d5537627"},"outputs":[{"data":{"text/plain":["'  He is more aloof than Bronco.\\n\\n### Output:\\n> The car is very fast and is more aloof than Bronco.\\n\\n### Programming Explanation:\\n\\n### [Le'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["json_data[0][\"base_model\"]"]},{"cell_type":"code","execution_count":6,"id":"7fdb842b","metadata":{},"outputs":[{"data":{"text/plain":["'Clouds that form when rain and thunderstorms combine are called wet-lift clouds.'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["json_data[1][\"finetuned_model\"]"]},{"cell_type":"markdown","id":"39a55283-7d51-4136-ba60-f799d49f4098","metadata":{"id":"39a55283-7d51-4136-ba60-f799d49f4098"},"source":["- Now, let's try LitGPT to compare the model responses (we only evaluate the first 5 responses for a visual comparison):"]},{"cell_type":"code","execution_count":7,"id":"0fb33cd2","metadata":{"id":"0fb33cd2"},"outputs":[],"source":["from litgpt import LLM\n","\n","llm = LLM.load(\"meta-llama/Meta-Llama-3-8B-Instruct\")"]},{"cell_type":"code","execution_count":10,"id":"3552bdfb-7511-42ac-a9ec-da672e2a5468","metadata":{"id":"3552bdfb-7511-42ac-a9ec-da672e2a5468"},"outputs":[],"source":["from tqdm import tqdm\n","\n","\n","def generate_model_scores(json_data, json_key):\n","    scores = []\n","    for entry in tqdm(json_data, desc=f\"Scoring entries ({json_key})\"):\n","        prompt = (\n","            f\"Given the input `{format_input(entry)}` \"\n","            f\"and correct output `{entry['output']}`, \"\n","            f\"score the model response `{entry[json_key]}`\"\n","            f\" on a scale from 0 to 100, where 100 is the best score. \"\n","            f\"Respond with the integer number only.\"\n","        )\n","        score = llm.generate(prompt, max_new_tokens=50)\n","        try:\n","            scores.append(int(score))\n","        except ValueError:\n","            continue\n","\n","    return scores"]},{"cell_type":"markdown","id":"5d967fa1","metadata":{"id":"5d967fa1"},"source":["# Exercise: Evaluate the LLMs\n","\n","- Now using the `generate_model_scores` function above, evaluate the finetuned (`base_model`) and non-finetuned model (`finetuned_model`)\n","- Apply this evaluation to the whole dataset and compute the average score of each model"]},{"cell_type":"code","execution_count":11,"id":"3487eff6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Scoring entries (base_model): 100%|██████████| 165/165 [02:23<00:00,  1.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model: base_model | score count: 124/165 | average score: 75.35483870967742\n"]},{"name":"stderr","output_type":"stream","text":["Scoring entries (finetuned_model): 100%|██████████| 165/165 [00:49<00:00,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["Model: finetuned_model | score count: 157/165 | average score: 76.828025477707\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model_scores = {\n","    \"base_model\": [],\n","    \"finetuned_model\": []\n","}\n","\n","for model in [\"base_model\", \"finetuned_model\"]:\n","    model_scores[model] = generate_model_scores(json_data, model)\n","    print(\n","        f\"Model: {model} | \"\n","        f\"score count: {len(model_scores[model])}/{len(json_data)} | \"\n","        f\"average score: {sum(model_scores[model]) / len(model_scores[model])}\"\n","    )"]},{"cell_type":"markdown","id":"5b14f7a6-1c36-43da-865c-32bd762f8458","metadata":{"id":"5b14f7a6-1c36-43da-865c-32bd762f8458"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# Solution"]},{"cell_type":"code","execution_count":null,"id":"89b3c452-9b14-4854-9ea7-734a882089c9","metadata":{"id":"89b3c452-9b14-4854-9ea7-734a882089c9","outputId":"0ec79753-c5dd-4525-a8c4-ee49880667fd"},"outputs":[{"name":"stderr","output_type":"stream","text":["Scoring entries: 100%|██████████| 165/165 [00:30<00:00,  5.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","response_before\n","Number of scores: 161 of 165\n","Average score: 84.02\n","\n"]},{"name":"stderr","output_type":"stream","text":["Scoring entries: 100%|██████████| 165/165 [00:29<00:00,  5.58it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","response_after\n","Number of scores: 160 of 165\n","Average score: 81.88\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["for model in (\"base_model\", \"finetuned_model\"):\n","\n","    scores = generate_model_scores(json_data, model)\n","    print(f\"\\n{model}\")\n","    print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n","    print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/rasbt/LLM-workshop-2024/blob/main/06_finetuning/06_part-1.ipynb","timestamp":1720562452959}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":5}

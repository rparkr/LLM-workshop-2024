{"cells":[{"cell_type":"markdown","id":"45398736-7e89-4263-89c8-92153baff553","metadata":{"id":"45398736-7e89-4263-89c8-92153baff553"},"source":["**LLM Workshop 2024 by Sebastian Raschka**\n","\n","This code is based on *Build a Large Language Model (From Scratch)*, [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)"]},{"cell_type":"markdown","id":"J5AgJeWfQCuk","metadata":{"id":"J5AgJeWfQCuk"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"id":"qNhatW3l5nyH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151397,"status":"ok","timestamp":1720558721803,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"qNhatW3l5nyH","outputId":"eb7676aa-da94-4640-ef08-ee1c1d1fb8b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.7/160.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.3/205.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# Requirements from: https://github.com/rasbt/LLM-workshop-2024/blob/main/requirements.txt\n","requirements = \"\"\"\n","# torch >= 2.0.1\n","tiktoken >= 0.5.1\n","# matplotlib >= 3.7.1\n","# numpy >= 1.24.3\n","# tensorflow >= 2.15.0\n","# tqdm >= 4.66.1\n","# numpy >= 1.25, < 2.0\n","# pandas >= 2.2.1\n","psutil >= 5.9.5\n","litgpt[all] >= 0.4.1\n","\"\"\"\n","\n","with open(\"requirements.txt\", mode=\"wt\") as f:\n","    f.write(requirements)\n","\n","%pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":null,"id":"6KFw-81yqT2k","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1720558737438,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"6KFw-81yqT2k","outputId":"417afef6-4463-4ac2-de18-7cc1e6ca608d"},"outputs":[{"data":{"text/plain":["ModuleSpec(name='tiktoken', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78b0daeed390>, origin='/usr/local/lib/python3.10/dist-packages/tiktoken/__init__.py', submodule_search_locations=['/usr/local/lib/python3.10/dist-packages/tiktoken'])"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Check if the tiktoken package is installed in the current environment\n","import importlib\n","importlib.util.find_spec(\"tiktoken\")"]},{"cell_type":"markdown","id":"vgtRAPoN6ixW","metadata":{"id":"vgtRAPoN6ixW"},"source":["Add supplementary Python modules from Sebastian Raschka's training material"]},{"cell_type":"code","execution_count":1,"id":"C45nepme6Waq","metadata":{"id":"C45nepme6Waq"},"outputs":[],"source":["import requests\n","session = requests.Session()\n","with open(\"load_pretrained_weights.py\", \"wt\", encoding=\"utf-8\") as f:\n","    response = session.get(\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/05_weightloading/supplementary.py\")\n","    f.write(response.text)\n","\n","with open(\"gpt_download.py\", \"wt\", encoding=\"utf-8\") as f:\n","    response = session.get(\"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/05_weightloading/gpt_download.py\")\n","    f.write(response.text)"]},{"cell_type":"markdown","id":"66dd524e-864c-4012-b0a2-ccfc56e80024","metadata":{"id":"66dd524e-864c-4012-b0a2-ccfc56e80024"},"source":["# 5) Loading pretrained weights (part 1)"]},{"cell_type":"code","execution_count":2,"id":"07a57fb9-f69e-44ca-ab29-3537fa5c0157","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1720558748761,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"07a57fb9-f69e-44ca-ab29-3537fa5c0157","outputId":"4707036e-47fe-4c68-a67d-cf4e94fae866"},"outputs":[{"name":"stdout","output_type":"stream","text":["matplotlib version: 3.8.2\n","numpy version: 1.26.4\n","tiktoken version: 0.7.0\n","torch version: 2.2.1+cu121\n"]}],"source":["from importlib.metadata import version\n","\n","pkgs = [\n","    \"matplotlib\",\n","    \"numpy\",\n","    \"tiktoken\",\n","    \"torch\",\n","]\n","for p in pkgs:\n","    print(f\"{p} version: {version(p)}\")"]},{"cell_type":"markdown","id":"83eb6c38-7278-40e0-bd9f-8a2b1feac3ec","metadata":{"id":"83eb6c38-7278-40e0-bd9f-8a2b1feac3ec"},"source":["- Previously, we only trained a small GPT-2 model using a very small short-story book for educational purposes\n","- Fortunately, we don't have to spend tens to hundreds of thousands of dollars to pretrain the model on a large pretraining corpus but can load pretrained weights (we start with the GPT-2 weights provided by OpenAI)\n","\n","<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/05_weightloading/figures/01.png?raw=1\" width=1000px>"]},{"cell_type":"markdown","id":"75cab892-a165-4f43-9601-f517bc212ab6","metadata":{"id":"75cab892-a165-4f43-9601-f517bc212ab6"},"source":["- First, some boilerplate code to download the files from OpenAI and load the weights into Python\n","- Since OpenAI used [TensorFlow](https://www.tensorflow.org/), we will have to install and use TensorFlow for loading the weights; [tqdm](https://github.com/tqdm/tqdm) is a progress bar library\n","- Uncomment and run the next cell to install the required libraries"]},{"cell_type":"code","execution_count":null,"id":"fb9fdf02-972a-444e-bf65-8ffcaaf30ce8","metadata":{"id":"fb9fdf02-972a-444e-bf65-8ffcaaf30ce8"},"outputs":[],"source":["# pip install tensorflow tqdm"]},{"cell_type":"code","execution_count":3,"id":"a0747edc-559c-44ef-a93f-079d60227e3f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1720558753380,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"a0747edc-559c-44ef-a93f-079d60227e3f","outputId":"3fb55854-7cc9-4432-bf76-a6372c1727b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow version: 2.16.2\n","tqdm version: 4.66.4\n"]}],"source":["print(\"TensorFlow version:\", version(\"tensorflow\"))\n","print(\"tqdm version:\", version(\"tqdm\"))"]},{"cell_type":"code","execution_count":4,"id":"c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed","metadata":{"id":"c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-10 03:20:36.250663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-09-10 03:20:36.342735: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-09-10 03:20:36.343464: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-09-10 03:20:36.504205: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-09-10 03:20:37.460882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["# Relative import from the gpt_download.py contained in this folder\n","from gpt_download import download_and_load_gpt2"]},{"cell_type":"markdown","id":"ff76a736-6f9f-4328-872e-f89a7b70a2cc","metadata":{"id":"ff76a736-6f9f-4328-872e-f89a7b70a2cc"},"source":["- We can then download the model weights for the 124 million parameter model as follows:"]},{"cell_type":"code","execution_count":5,"id":"76271dd7-108d-4f5b-9c01-6ae0aac4b395","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10745,"status":"ok","timestamp":1720558779006,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"76271dd7-108d-4f5b-9c01-6ae0aac4b395","outputId":"03a159a7-8a29-41ab-8fad-29294cb12711"},"outputs":[{"name":"stdout","output_type":"stream","text":["File already exists and is up-to-date: gpt2/124M/checkpoint\n","File already exists and is up-to-date: gpt2/124M/encoder.json\n","File already exists and is up-to-date: gpt2/124M/hparams.json\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n","File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"]},{"name":"stderr","output_type":"stream","text":["2024-09-10 03:20:45.954735: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n"]}],"source":["settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"]},{"cell_type":"code","execution_count":6,"id":"b1a31951-d971-4a6e-9c43-11ee1168ec6a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143,"status":"ok","timestamp":1720558783444,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"b1a31951-d971-4a6e-9c43-11ee1168ec6a","outputId":"23ff2e22-7243-4b51-a58b-b44f08c30fc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"]}],"source":["print(\"Settings:\", settings)"]},{"cell_type":"code","execution_count":7,"id":"857c8331-130e-46ba-921d-fa35d7a73cfe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1720558785392,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"857c8331-130e-46ba-921d-fa35d7a73cfe","outputId":"6857e87f-0975-424c-888f-20aece7c37dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"]}],"source":["print(\"Parameter dictionary keys:\", params.keys())"]},{"cell_type":"code","execution_count":8,"id":"c48dac94-8562-4a66-84ef-46c613cdc4cd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153,"status":"ok","timestamp":1720558793581,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"c48dac94-8562-4a66-84ef-46c613cdc4cd","outputId":"1f1d440f-0c09-4822-88bd-acffc7c15ae0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n","   0.04531523]\n"," [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n","   0.04318958]\n"," [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n","  -0.08785918]\n"," ...\n"," [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n","  -0.06952604]\n"," [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n","  -0.02245961]\n"," [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n","   0.12067825]]\n","Token embedding weight tensor dimensions: (50257, 768)\n"]}],"source":["print(params[\"wte\"])\n","print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"]},{"cell_type":"markdown","id":"466e100c-294e-4afc-a70a-2f398ac4c104","metadata":{"id":"466e100c-294e-4afc-a70a-2f398ac4c104"},"source":["- Alternatively, \"355M\", \"774M\", and \"1558M\" are also supported `model_size` arguments\n","- The difference between these differently sized models is summarized in the figure below:"]},{"cell_type":"markdown","id":"20f19d32-5aae-4176-9f86-f391672c8f0d","metadata":{"id":"20f19d32-5aae-4176-9f86-f391672c8f0d"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/05_weightloading/figures/02.png?raw=1\" width=800px>"]},{"cell_type":"markdown","id":"ea6e5076-f08d-41fc-bd8b-1cfe53538f41","metadata":{"id":"ea6e5076-f08d-41fc-bd8b-1cfe53538f41"},"source":["- Above, we loaded the 124M GPT-2 model weights into Python, however we still need to transfer them into our `GPTModel` instance\n","- First, we initialize a new GPTModel instance\n","- Note that the original GPT model initialized the linear layers for the query, key, and value matrices in the multi-head attention module with bias vectors, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting `qkv_bias` to `True` in our implementation, too\n","- We are also using the `1024` token context length that was used by the original GPT-2 model(s)"]},{"cell_type":"code","execution_count":9,"id":"9fef90dd-0654-4667-844f-08e28339ef7d","metadata":{"id":"9fef90dd-0654-4667-844f-08e28339ef7d"},"outputs":[],"source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Vocabulary size\n","    \"context_length\": 256, # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,        # Embedding dimension\n","    \"n_heads\": 12,         # Number of attention heads\n","    \"n_layers\": 12,        # Number of layers\n","    \"drop_rate\": 0.1,      # Dropout rate\n","    \"qkv_bias\": False      # Query-key-value bias\n","}\n","\n","\n","# Define model configurations in a dictionary for compactness\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}\n","\n","# Copy the base configuration and update with specific model settings\n","model_name = \"gpt2-small (124M)\"  # Example model name\n","NEW_CONFIG = GPT_CONFIG_124M.copy()\n","NEW_CONFIG.update(model_configs[model_name])\n","NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})"]},{"cell_type":"code","execution_count":10,"id":"c20cdc14-684d-4f77-a786-6646f66bae81","metadata":{"id":"c20cdc14-684d-4f77-a786-6646f66bae81"},"outputs":[],"source":["from load_pretrained_weights import GPTModel\n","\n","gpt = GPTModel(NEW_CONFIG)\n","gpt.eval();"]},{"cell_type":"markdown","id":"272f29ac-8342-4b3d-a57d-9b0166ced314","metadata":{"id":"272f29ac-8342-4b3d-a57d-9b0166ced314"},"source":["- The next task is to assign the OpenAI weights to the corresponding weight tensors in our `GPTModel` instance"]},{"cell_type":"code","execution_count":11,"id":"f9a92229-c002-49a6-8cfb-248297ad8296","metadata":{"id":"f9a92229-c002-49a6-8cfb-248297ad8296"},"outputs":[],"source":["def assign(left, right):\n","    if left.shape != right.shape:\n","        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n","    return torch.nn.Parameter(torch.tensor(right))"]},{"cell_type":"code","execution_count":12,"id":"f22d5d95-ca5a-425c-a9ec-fc432a12d4e9","metadata":{"id":"f22d5d95-ca5a-425c-a9ec-fc432a12d4e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using: cuda\n"]}],"source":["import torch\n","import numpy as np\n","\n","def load_weights_into_gpt(gpt, params):\n","    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n","    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n","\n","    for b in range(len(params[\"blocks\"])):\n","        q_w, k_w, v_w = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.weight = assign(\n","            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n","        gpt.trf_blocks[b].att.W_key.weight = assign(\n","            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n","        gpt.trf_blocks[b].att.W_value.weight = assign(\n","            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n","\n","        q_b, k_b, v_b = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.bias = assign(\n","            gpt.trf_blocks[b].att.W_query.bias, q_b)\n","        gpt.trf_blocks[b].att.W_key.bias = assign(\n","            gpt.trf_blocks[b].att.W_key.bias, k_b)\n","        gpt.trf_blocks[b].att.W_value.bias = assign(\n","            gpt.trf_blocks[b].att.W_value.bias, v_b)\n","\n","        gpt.trf_blocks[b].att.out_proj.weight = assign(\n","            gpt.trf_blocks[b].att.out_proj.weight,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].att.out_proj.bias = assign(\n","            gpt.trf_blocks[b].att.out_proj.bias,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[0].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[0].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n","        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[2].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[2].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].norm1.scale = assign(\n","            gpt.trf_blocks[b].norm1.scale,\n","            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n","        gpt.trf_blocks[b].norm1.shift = assign(\n","            gpt.trf_blocks[b].norm1.shift,\n","            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n","        gpt.trf_blocks[b].norm2.scale = assign(\n","            gpt.trf_blocks[b].norm2.scale,\n","            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n","        gpt.trf_blocks[b].norm2.shift = assign(\n","            gpt.trf_blocks[b].norm2.shift,\n","            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n","\n","    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n","    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n","    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n","\n","\n","load_weights_into_gpt(gpt, params)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using:\", device)\n","gpt.to(device);"]},{"cell_type":"markdown","id":"4f7472cb-54dc-4311-96d8-b2694f885cee","metadata":{"id":"4f7472cb-54dc-4311-96d8-b2694f885cee"},"source":["- If the model is loaded correctly, we can use it to generate new text using our previous `generate` function:"]},{"cell_type":"code","execution_count":13,"id":"1f690253-f845-4347-b7b6-43fabbd2affa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2620,"status":"ok","timestamp":1720558962467,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"1f690253-f845-4347-b7b6-43fabbd2affa","outputId":"a586dfdf-4fdd-4cae-9dd8-300fe3391426"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand\n"]}],"source":["import tiktoken\n","from load_pretrained_weights import (\n","    generate_text_simple,\n","    text_to_token_ids,\n","    token_ids_to_text\n",")\n","\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","torch.manual_seed(123)\n","\n","token_ids = generate_text_simple(\n","    model=gpt,\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"]},{"cell_type":"code","execution_count":14,"id":"L05W2SP9Tr8t","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8633,"status":"ok","timestamp":1720559332196,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"L05W2SP9Tr8t","outputId":"0cdef9f2-9f0b-461c-8e52-b57069c28890"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output text:\n"," A summary of Newton's laws of motion is:\n","1. Law of inertia: The motion of the earth is proportional to the velocity of the sun.\n","2. Law of inertia: The motion of the earth is proportional to the\n"]}],"source":["token_ids = generate_text_simple(\n","    model=gpt,\n","    idx=text_to_token_ids(\n","        (\n","            \"A summary of Newton's laws of motion is:\"\n","            \"\\n1. Law of inertia:\"\n","        ),\n","        tokenizer).to(device),\n","        max_new_tokens=30,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"]},{"cell_type":"markdown","id":"6d079f98-a7c4-462e-8416-5a64f670861c","metadata":{"id":"6d079f98-a7c4-462e-8416-5a64f670861c"},"source":["- We know that we loaded the model weights correctly because the model can generate coherent text; if we made even a small mistake, the mode would not be able to do that"]},{"cell_type":"markdown","id":"1a30d071-30cd-43df-ba83-a6b162593b19","metadata":{"id":"1a30d071-30cd-43df-ba83-a6b162593b19"},"source":["# Exercise 1: Trying larger LLMs"]},{"cell_type":"markdown","id":"9a459f5a-578e-4145-8bac-aad43915de0d","metadata":{"id":"9a459f5a-578e-4145-8bac-aad43915de0d"},"source":["- Load one of the larger LLMs and see how the output quality compares\n","- Ask it to answer specific instructions, for example to summarize text or correct the spelling of a sentence\n","\n","---\n","\n","I am using Lightning AI Studios with an L4 GPU and 16 CPUs."]},{"cell_type":"code","execution_count":15,"id":"4fa45afb","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 107kiB/s]\n","encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.14MiB/s]\n","hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 144kiB/s]\n","model.ckpt.data-00000-of-00001: 100%|██████████| 6.23G/6.23G [02:07<00:00, 48.8MiB/s] \n","model.ckpt.index: 100%|██████████| 20.7k/20.7k [00:00<00:00, 726kiB/s]\n","model.ckpt.meta: 100%|██████████| 1.84M/1.84M [00:00<00:00, 5.66MiB/s]\n","vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 3.04MiB/s]\n","2024-09-10 03:26:25.928738: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 321644800 exceeds 10% of free system memory.\n"]}],"source":["# Use the largest model: 1558 M parameters\n","# This takes about 2.5 minutes on Lightning AI Studios\n","settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n","\n","# Define model configurations in a dictionary for compactness\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}\n","\n","# Copy the base configuration and update with specific model settings\n","model_name = \"gpt2-xl (1558M)\"  # Example model name\n","NEW_CONFIG = GPT_CONFIG_124M.copy()\n","NEW_CONFIG.update(model_configs[model_name])\n","NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n","gpt = GPTModel(NEW_CONFIG)\n","gpt.eval()\n","\n","load_weights_into_gpt(gpt, params)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using:\", device)\n","gpt.to(device)"]},{"cell_type":"code","execution_count":18,"id":"8ec081d3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output text:\n"," A summary of Newton's laws of motion is:\n","1. Law of inertia: The force of an object on another object is directly proportional to the product of the masses of the objects.\n","2. Law of conservation of energy: Energy is conserved.\n","3. Law of conservation of momentum: The momentum of an object is directly proportional to the product of the masses of the objects.\n","4. Law of conservation of angular momentum: The angular momentum of an object is directly proportional to the product of the masses of the objects.\n","5. Law of conservation of angular momentum: The angular momentum of an object is directly proportional to the product of the masses of the objects.\n","6. Law of conservation of angular momentum: The angular momentum of an object is directly proportional to the product of the masses of the objects.\n","7. Law of conservation of angular momentum: The angular momentum of an object is directly proportional to the product of the masses of the objects.\n","8. Law of conservation of angular momentum: The angular momentum of an object is directly proportional to the product of the masses of the objects.\n","9. Law of conservation of angular momentum: The angular momentum of an object is directly proportional to the product of the masses of the objects.\n","10. Law of conservation of angular momentum: The angular momentum of an object is directly proportional to\n"]}],"source":["token_ids = generate_text_simple(\n","    model=gpt,\n","    idx=text_to_token_ids(\n","        (\n","            \"A summary of Newton's laws of motion is:\"\n","            \"\\n1. Law of inertia:\"\n","        ),\n","        tokenizer).to(device),\n","        max_new_tokens=256,\n","    context_size=NEW_CONFIG[\"context_length\"]\n",")\n","\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"]},{"cell_type":"markdown","id":"690b1b5e","metadata":{},"source":["Ask the LLM to summarize a page from Wikipedia. For convenience, I use the [`wikipedia-api`](https://github.com/martin-majlis/Wikipedia-API) Python interface to the Wikipedia API (which we could also access using, e.g., `httpx` or `requests`)."]},{"cell_type":"code","execution_count":null,"id":"2b22b42d","metadata":{},"outputs":[],"source":["%pip install wikipedia-api"]},{"cell_type":"code","execution_count":21,"id":"357d453e","metadata":{},"outputs":[],"source":["# Fetch an article from Wikipedia and ask the LLM to summarize it\n","import wikipediaapi\n","wiki = wikipediaapi.Wikipedia(\n","    user_agent=\"LLMs from Scratch (ryan.parker2@outlook.com)\",\n","    language='en',\n","    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",")\n","\n","page = wiki.page(\"Hubble_Deep_Field\")"]},{"cell_type":"code","execution_count":25,"id":"a0ecab51","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The Hubble Deep Field (HDF) is an image of a small region in the constellation Ursa Major, constructed from a series of observations by the Hubble Space Telescope. It covers an area about 2.6 arcminutes on a side, about one 24-millionth of the whole sky, which is equivalent in angular size to a tennis ball at a distance of 100 metres. The image was assembled from 342 separate exposures taken with the Space Telescope's Wide Field and Planetary Camera 2 over ten consecutive days between December 18 and 28, 1995.\n","The field is so small that only a few foreground stars in the Milky Way lie within it; thus, almost all of the 3,000 objects in the image are galaxies, some of which are among the youngest and most distant known. By revealing such large numbers of very young galaxies, the HDF has become a landmark image in the study of the early universe.\n","Three years after the HDF observations were taken, a region in the south celestial hemisphere was imaged in a similar way and named the Hubble Deep Field South. The similarities between the two regions strengthened the belief that the universe is uniform over large scales and that the Earth occupies a typical region in the Universe (the cosmological principle). A wider but shallower survey was also made as part of the Great Observatories Origins Deep Survey. In 2004 a deeper image, known as the Hubble Ultra-Deep Field (HUDF), was constructed from a few months of light exposure. The HUDF image was at the time the most sensitive astronomical image ever made at visible wavelengths, and it remained so until the Hubble eXtreme Deep Field (XDF) was released in 2012.\n"]}],"source":["print(page.summary)"]},{"cell_type":"code","execution_count":26,"id":"79610e72","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output text:\n"," Summarize the following: The Hubble Deep Field (HDF) is an image of a small region in the constellation Ursa Major, constructed from a series of observations by the Hubble Space Telescope. It covers an area about 2.6 arcminutes on a side, about one 24-millionth of the whole sky, which is equivalent in angular size to a tennis ball at a distance of 100 metres. The image was assembled from 342 separate exposures taken with the Space Telescope's Wide Field and Planetary Camera 2 over ten consecutive days between December 18 and 28, 1995.\n","The field is so small that only a few foreground stars in the Milky Way lie within it; thus, almost all of the 3,000 objects in the image are galaxies, some of which are among the youngest and most distant known. By revealing such large numbers of very young galaxies, the HDF has become a landmark image in the study of the early universe.\n","Three years after the HDF observations were taken, a region in the south celestial hemisphere was imaged in a similar way and named the Hubble Deep Field South. The similarities between the two regions strengthened the belief that the universe is uniform over large scales and that the Earth occupies a typical region in the Universe (the cosmological principle). A wider but shallower survey was also made as part of the Great Observatories Origins Deep Survey. In 2004 a deeper image, known as the Hubble Ultra-Deep Field (HUDF), was constructed from a few months of light exposure. The HUDF image was at the time the most sensitive astronomical image ever made at visible wavelengths, and it remained so until the Hubble eXtreme Deep Field (XDF) was released in 2012.\n","\n","Conception\n","One of the key aims of the astronomers who designed the Hubble Space Telescope was to use its high optical resolution to study distant galaxies to a level of detail that was not possible from the ground. Positioned above the atmosphere, Hubble avoids atmospheric airglow allowing it to take more sensitive visible and ultraviolet light images than can be obtained with seeing-limited ground-based telescopes (when good adaptive optics correction at visible wavelengths becomes possible, 10 m ground-based telescopes may become competitive). Although the telescope's mirror suffered from spherical aberration when the telescope was launched in 1990, it could still be used to take images of more distant galaxies than had previously been obtainable. Because light takes billions of years to reach Earth from very distant galaxies, we see them as they were billions of years ago; thus, extending the scope of such research to increasingly distant galaxies allows a better understanding of how they evolve.\n","After the spherical aberration was corrected during Space Shuttle mission STS-61 in 1993, the improved imaging capabilities of the telescope were used to study increasingly distant and faint galaxies. The Medium Deep Survey (MDS) used the Wide Field and Planetary Camera 2 (WFPC2) to take deep images of random fields while other instruments were being used for scheduled observations. At the same time, other dedicated programs focused on galaxies that were already known through ground-based observation. All of these studies revealed substantial differences between the properties of galaxies today and those that existed several billion years ago.\n","Up to 10% of the HST's observation time is designated as Director's Discretionary (DD) Time, and is typically awarded to astronomers who wish to study unexpected transient phenomena, such as supernovae. Once Hubble's corrective optics were shown to be performing well, Robert Williams, the then-director of the Space Telescope Science Institute, decided to devote a substantial fraction of his DD time during 1995 to the study of distant galaxies. A special Institute Advisory Committee recommended that the WFPC2 be used to image a \"typical\" patch of sky at a high galactic latitude, using several optical filters. A working group was set up to develop and implement the project.\n","\n","Target selection\n","The field selected for the observations needed to fulfill several criteria. It had to be at a high galactic latitude because dust and obscuring matter in the plane of the Milky Way's disc prevents observations of distant galaxies at low galactic latitudes (see Zone of Avoidance). The target field had to avoid known bright sources of visible light (such as foreground stars), and infrared, ultraviolet, and X-ray emissions, to facilitate later studies at many wavelengths of the objects in the deep field, and also needed to be in a region with a low background infrared cirrus, the diffuse, wispy infrared emission believed to be caused by warm dust grains in cool clouds of hydrogen gas (H I regions).\n","These criteria restricted the field of potential target areas. It was decided that the target should be in Hubble's continuous viewing zones: the areas of sky that are not occulted by the Earth or the moon during Hubble's orbit. The working group decided to concentrate on the northern continuous viewing zone, so that northern-hemisphere telescopes such as the Keck telescopes, the Kitt Peak National Observatory telescopes, and the Very Large Array (VLA) could conduct follow-up observations.\n","Twenty fields satisfying these criteria were identified, from which three optimal candidate fields were selected, all within the constellation of Ursa Major. Radio snapshot observations with the VLA ruled out one of these fields because it contained a bright radio source, and the final decision between the other two was made on the basis of the availability of guide stars near the field: Hubble observations normally require a pair of nearby stars on which the telescope's Fine Guidance Sensors can lock during an exposure, but given the importance of the HDF observations, the working group required a second set of back-up guide stars. The field that was eventually selected is located at a right ascension of 12h 36m 49.4s and a declination of +62° 12′ 58″; it is about 2.6 arcminutes in width, or 1/12 the width of the Moon. The area is about 1/24,000,000 of the total area of the sky.\n","\n","Observations\n","Once a field was selected, an observing strategy was developed. An important decision was to determine which filters the observations would use; WFPC2 is equipped with 48 filters, including narrowband filters isolating particular emission lines of astrophysical interest, and broadband filters useful for the study of the colors of stars and galaxies. The choice of filters to be used for the HDF depended on the throughput of each filter—the total proportion of light that it allows through—and the spectral coverage available. Filters with bandpasses overlapping as little as possible were desirable.\n","In the end, four broadband filters were chosen, centred at wavelengths of 300 nm (near-ultraviolet), 450 nm (blue light), 606 nm (red light) and 814 nm (near-infrared). Because the quantum efficiency of Hubble's detectors at 300 nm wavelength is quite low, the noise in observations at this wavelength is primarily due to CCD noise rather than sky background; thus, these observations could be conducted at times when high background noise would have harmed the efficiency of observations in other passbands.\n","Between December 18 and 28, 1995—during which time Hubble orbited the Earth about 150 times—342 images of the target area in the chosen filters were taken. The total exposure times at each wavelength were 42.7 hours (300 nm), 33.5 hours (450 nm), 30.3 hours (606 nm) and 34.3 hours (814 nm), divided into 342 individual exposures to prevent significant damage to individual images by cosmic rays, which cause bright streaks to appear when they strike CCD detectors. A further 10 Hubble orbits were used to make short exposures of flanking fields to aid follow-up observations by other instruments.\n","\n","Data processing\n","The production of a final combined image at each wavelength was a complex process. Bright pixels caused by cosmic ray impacts during exposures were removed by comparing exposures of equal length taken one after the other, and identifying pixels that were affected by cosmic rays in one exposure but not the other. Trails of space debris and artificial satellites were present in the original images, and were carefully removed.\n","Scattered light from the Earth was evident in about a quarter of the data frames, creating a visible \"X\" pattern on the images. This was removed by taking an image affected by scattered light, aligning it with an unaffected image, and subtracting the unaffected image from the affected one. The resulting image was smoothed, and could then be subtracted from the bright frame. This procedure removed almost all of the scattered light from the affected images.\n","Once the 342 individual images were cleaned of cosmic-ray hits and corrected for scattered light, they had to be combined. Scientists involved in the HDF observations pioneered a technique called 'drizzling', in which the pointing of the telescope was varied minutely between sets of exposures. Each pixel on the WFPC2 CCD chips recorded an area of sky 0.09 arcseconds across, but by changing the direction in which the telescope was pointing by less than that between exposures, the resulting images were combined using sophisticated image-processing techniques to yield a final angular resolution better than this value. The HDF images produced at each wavelength had final pixel sizes of 0.03985 arcseconds.\n","The data processing yielded four monochrome images (at 300 nm, 450 nm, 606 nm and 814 nm), one at each wavelength. One image was designated as red (814 nm), the second as green (606 nm) and the third as blue (450 nm), and the three images were combined to give a color image. Because the wavelengths at which the images were taken do not correspond to the wavelengths of red, green and blue light, the colors in the final image only give an approximate representation of the actual colors of the galaxies in the image; the choice of filters for the HDF (and the majority of Hubble images) was primarily designed to maximize the scientific utility of the observations rather than to create colors corresponding to what the human eye would actually perceive.\n","\n","Contents\n","The final images were released at a meeting of the American Astronomical Society in January 1996, and revealed a plethora of distant, faint galaxies. About 3,000 distinct galaxies could be identified in the images, with both irregular and spiral galaxies clearly visible, although some galaxies in the field are only a few pixels across. In all, the HDF is thought to contain fewer than twenty galactic foreground stars; by far the majority of objects in the field are distant galaxies.\n","There are about fifty blue point-like objects in the HDF. Many seem to be associated with nearby galaxies, which together form chains and arcs: these are likely to be regions of intense star formation. Others may be distant quasars. Astronomers initially ruled out the possibility that some of the point-like objects are white dwarfs, because they are too blue to be consistent with theories of white dwarf evolution prevalent at the time. However, more recent work has found that many white dwarfs become bluer as they age, lending support to the idea that the HDF might contain white dwarfs.\n","\n","Scientific results\n","The HDF data provided extremely rich material for cosmologists to analyse and by late 2014 the associated scientific paper for the image had received over 900 citations. One of the most fundamental findings was the discovery of large numbers of galaxies with high redshift values.\n","As the Universe expands, more distant objects recede from the Earth faster, in what is called the Hubble Flow. The light from very distant galaxies is significantly affected by the cosmological redshift. While quasars with high redshifts were known, very few galaxies with redshifts greater than one were known before the HDF images were produced. The HDF, however, contained many galaxies with redshifts as high as six, corresponding to distances of about 12 billion light-years. Due to redshift the most distant objects in the HDF (Lyman-break galaxies) are not actually visible in the Hubble images; they can only be detected in images of the HDF taken at longer wavelengths by ground-based telescopes. One of the first observations planned for the James Webb Space Telescope was a mid-infrared image of the Hubble Ultra-Deep Field.\n","\n","The HDF galaxies contained a considerably larger proportion of disturbed and irregular galaxies than the local universe; galaxy collisions and mergers were more common in the young universe as it was much smaller than today. It is believed that giant elliptical galaxies form when spirals and irregular galaxies collide.\n","The wealth of galaxies at different stages of their evolution also allowed astronomers to estimate the variation in the rate of star formation over the lifetime of the Universe. While estimates of the redshifts of HDF galaxies are somewhat crude, astronomers believe that star formation was occurring at its maximum rate 8–10 billion years ago, and has decreased by a factor of about 10 since then.\n","Another important result from the HDF was the very small number of foreground stars present.  For years astronomers had been puzzling over the nature of dark matter, mass which seems to be undetectable but which observations implied made up about 85% of all matter in the Universe by mass. One theory was that dark matter might consist of Massive Astrophysical Compact Halo Objects (MACHOs)—faint but massive objects such as red dwarfs and planets in the outer regions of galaxies. The HDF showed, however, that there were not significant numbers of red dwarfs in the outer parts of our galaxy.\n","\n","Multifrequency followup\n","Very-high redshift objects (Lyman-break galaxies) cannot be seen in visible light and generally are detected in infrared or submillimetre wavelength surveys of the HDF instead. Observations with the Infrared Space Observatory (ISO) indicated infrared emission from 13 galaxies visible in the optical images, attributed to large quantities of dust associated with intense star formation. Infrared observations have also been made with the Spitzer Space Telescope. Submillimeter observations of the field have been made with SCUBA on the James Clerk Maxwell Telescope, initially detecting 5 sources, although with very low resolution. Observations have also been made with the Subaru telescope in Hawaii.\n","X-ray observations by the Chandra X-ray Observatory revealed six sources in the HDF, which were found to correspond to three elliptical galaxies, one spiral galaxy, one active galactic nucleus and one extremely red object, thought to be a distant galaxy containing a large amount of dust absorbing its blue light emissions.\n","Ground-based radio images taken using the VLA revealed seven radio sources in the HDF, all of which correspond to galaxies visible in the optical images. The field has also been surveyed with the Westerbork Synthesis Radio Telescope and the MERLIN array of radio telescopes at 1.4 GHz; the combination of VLA and MERLIN maps made at wavelengths of 3.5 and 20 cm have located 16 radio sources in the HDF-N field, with many more in the flanking fields. Radio images of some individual sources in the field have been made with the European VLBI Network at 1.6 GHz with a higher resolution than the Hubble maps.\n","\n","Subsequent HST observations\n","An HDF counterpart in the southern celestial hemisphere was created in 1998: the HDF-South (HDF-S). Created using a similar observing strategy, the HDF-S was very similar in appearance to the original HDF. This supports the cosmological principle that at its largest scale the Universe is homogeneous. The HDF-S survey used the Space Telescope Imaging Spectrograph (STIS) and the Near Infrared Camera and Multi-Object Spectrometer (NICMOS) instruments installed on the HST in 1997; the region of the original Hubble Deep Field (HDF-N) has since been re-observed several times using WFPC2, as well as by the NICMOS and STIS instruments. Several supernova events were detected by comparing the first and second epoch observations of the HDF-N.\n","A wider survey, but less sensitive, was carried out as part of the Great Observatories Origins Deep Survey; a section of this was then observed for longer to create the Hubble Ultra-Deep Field, which was the most sensitive optical deep field image for years until the Hubble eXtreme Deep Field was completed in 2012. Images from the Extreme Deep Field, or XDF, were released on September 26, 2012, to a number of media agencies. Images released in the XDF show galaxies which are now believed to have formed in the first 500 million years following the Big Bang.\n","\n","See also\n","List of deep fields\n","\n","Notes and references\n","Bibliography\n","External links\n"," Media related to Hubble Deep Field at Wikimedia Commons\n","\n","\"The Hubble Deep Field\". STScI. Main Hubble Deep Field website.\n","\"Hubble's Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years\". January 15, 1996. NASA's original press release.\n","\n","\"Hubble's Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years\". January 15, 1996. NASA's original press release. \"Hubble's Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years\". January 15, 1996. NASA's original press release. \"Hubble's Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years\". January 15, 1996. NASA's original press release. \"Hubble's Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years\". January 15, 1996. NASA's original press release. \"Hubble's Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years\". January 15, 1996. NASA's original press release. \"Hubble's Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years\". January 15, 1996. NASA's original press release. \"Hubble's Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years\". January 15, 1996. NASA's original press release. \"Hub\n"]}],"source":["token_ids = generate_text_simple(\n","    model=gpt,\n","    idx=text_to_token_ids(\n","         (\n","            f\"Summarize the following: {page.text}\"\n","            \"\\n\\n\" + (\"-\" * 50) + \"\\n\\n\"\n","            \"Summary:\\n\"\n","        ),\n","        tokenizer\n","    ).to(device),\n","    max_new_tokens=256,\n","    context_size=NEW_CONFIG[\"context_length\"]\n",")\n","\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"]},{"cell_type":"code","execution_count":29,"id":"6db757c2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output text:\n"," Summarize the following: The Hubble Deep Field (HDF) is an image of a small region in the constellation Ursa Major, constructed from a series of observations by the Hubble Space Telescope. It covers an area about 2.6 arcminutes on a side, about one 24-millionth of the whole sky, which is equivalent in angular size to a tennis ball at a distance of 100 metres. The image was assembled from 342 separate exposures taken with the Space Telescope's Wide Field and Planetary Camera 2 over ten consecutive days between December 18 and 28, 1995.\n","The field is so small that only a few foreground stars in the Milky Way lie within it; thus, almost all of the 3,000 objects in the image are galaxies, some of which are among the youngest and most distant known. By revealing such large numbers of very young galaxies, the HDF has become a landmark image in the study of the early universe.\n","Three years after the HDF observations were taken, a region in the south celestial hemisphere was imaged in a similar way and named the Hubble Deep Field South. The similarities between the two regions strengthened the belief that the universe is uniform over large scales and that the Earth occupies a typical region in the Universe (the cosmological principle). A wider but shallower survey was also made as part of the Great Observatories Origins Deep Survey. In 2004 a deeper image, known as the Hubble Ultra-Deep Field (HUDF), was constructed from a few months of light exposure. The HUDF image was at the time the most sensitive astronomical image ever made at visible wavelengths, and it remained so until the Hubble eXtreme Deep Field (XDF) was released in 2012.\n","\n","--------------------------------------------------\n","\n","Summary:\n","\n","The Hubble Deep Field (HDF) is an image of a small region in the constellation Ursa Major, constructed from a series of observations by the Hubble Space Telescope. It covers an area about 2.6 arcminutes on a side, about one 24-millionth of the whole sky, which is equivalent in angular size to a tennis ball at a distance of 100 metres. The image was assembled from 342 separate exposures taken with the Space Telescope's Wide Field and Planetary Camera 2 over ten consecutive days between December 18 and 28, 1995.\n","\n","The field is so small that only a few foreground stars in the Milky Way lie within it; thus, almost all of the 3,000 objects in the image are galaxies, some of which are among the youngest and most distant known. By revealing such large numbers of very young galaxies, the HDF has become a landmark image in the study of the early universe.\n","\n","Three years after the HDF observations were taken, a region in the south celestial hemisphere was imaged in a similar way and named the Hubble Deep Field South. The similarities between the two regions strengthened the belief that the universe is uniform over large scales and that the Earth occupies a typical region in the Universe (the cosmological principle). A wider\n"]}],"source":["token_ids = generate_text_simple(\n","    model=gpt,\n","    idx=text_to_token_ids(\n","        (\n","            f\"Summarize the following: {page.summary}\"\n","            \"\\n\\n\" + (\"-\" * 50) + \"\\n\\n\"\n","            \"Summary:\\n\"\n","        ),\n","        tokenizer\n","    ).to(device),\n","    max_new_tokens=256,\n","    context_size=NEW_CONFIG[\"context_length\"]\n",")\n","\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"]},{"cell_type":"markdown","id":"357d453e","metadata":{},"source":["---\n","\n","# 5) Loading pretrained weights (part 2; using LitGPT)"]},{"cell_type":"markdown","id":"4d617b8f-8493-4afa-8c91-f3d1ab79795b","metadata":{"id":"4d617b8f-8493-4afa-8c91-f3d1ab79795b"},"source":["- Now, we are loading the weights using an open-source library called LitGPT\n","- LitGPT is fundamentally similar to the LLM code we implemented previously, but it is much more sophisticated and supports more than 20 different LLMs (Mistral, Gemma, Llama, Phi, and more)\n","\n","# ⚡ LitGPT\n","\n","**20+ high-performance LLMs with recipes to pretrain, finetune, deploy at scale.**\n","\n","<pre>\n","✅ From scratch implementations     ✅ No abstractions    ✅ Beginner friendly   \n","✅ Flash attention                  ✅ FSDP               ✅ LoRA, QLoRA, Adapter\n","✅ Reduce GPU memory (fp4/8/16/32)  ✅ 1-1000+ GPUs/TPUs  ✅ 20+ LLMs            \n","</pre>\n","\n","## Basic usage:\n","\n","```\n","# ligpt [action] [model]\n","litgpt  download  meta-llama/Meta-Llama-3-8B-Instruct\n","litgpt  chat      meta-llama/Meta-Llama-3-8B-Instruct\n","litgpt  evaluate  meta-llama/Meta-Llama-3-8B-Instruct\n","litgpt  finetune  meta-llama/Meta-Llama-3-8B-Instruct\n","litgpt  pretrain  meta-llama/Meta-Llama-3-8B-Instruct\n","litgpt  serve     meta-llama/Meta-Llama-3-8B-Instruct\n","```\n","\n","\n","- You can learn more about LitGPT in the [corresponding GitHub repository](https://github.com/Lightning-AI/litgpt), that contains many tutorials, use cases, and examples\n"]},{"cell_type":"code","execution_count":null,"id":"b1f9508e","metadata":{"id":"b1f9508e"},"outputs":[],"source":["# pip install litgpt"]},{"cell_type":"code","execution_count":1,"id":"48cf71fa-af17-4c72-a6ab-f258a2b5a8ac","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1720559405126,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"48cf71fa-af17-4c72-a6ab-f258a2b5a8ac","outputId":"747c52e4-b838-4317-8395-ca4d897f3cdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["litgpt version: 0.4.3.dev0\n","torch version: 2.2.1+cu121\n"]}],"source":["from importlib.metadata import version\n","\n","pkgs = [\n","    \"litgpt\",\n","    \"torch\",\n","]\n","for p in pkgs:\n","    print(f\"{p} version: {version(p)}\")"]},{"cell_type":"markdown","id":"fe29baa9-c3b0-493d-94b4-eaa8146d6b3c","metadata":{"id":"fe29baa9-c3b0-493d-94b4-eaa8146d6b3c"},"source":["- First, let's see what LLMs are supported"]},{"cell_type":"code","execution_count":2,"id":"0ae8df66-f391-4266-b437-a1f601a6ac40","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11637,"status":"ok","timestamp":1720559421103,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"0ae8df66-f391-4266-b437-a1f601a6ac40","outputId":"9053012c-7334-4e9b-d4b5-b60fe4765fef"},"outputs":[{"name":"stdout","output_type":"stream","text":["repo_id: list\n","Please specify --repo_id <repo_id>. Available values:\n","codellama/CodeLlama-13b-hf\n","codellama/CodeLlama-13b-Instruct-hf\n","codellama/CodeLlama-13b-Python-hf\n","codellama/CodeLlama-34b-hf\n","codellama/CodeLlama-34b-Instruct-hf\n","codellama/CodeLlama-34b-Python-hf\n","codellama/CodeLlama-70b-hf\n","codellama/CodeLlama-70b-Instruct-hf\n","codellama/CodeLlama-70b-Python-hf\n","codellama/CodeLlama-7b-hf\n","codellama/CodeLlama-7b-Instruct-hf\n","codellama/CodeLlama-7b-Python-hf\n","databricks/dolly-v2-12b\n","databricks/dolly-v2-3b\n","databricks/dolly-v2-7b\n","EleutherAI/pythia-1.4b\n","EleutherAI/pythia-1.4b-deduped\n","EleutherAI/pythia-12b\n","EleutherAI/pythia-12b-deduped\n","EleutherAI/pythia-14m\n","EleutherAI/pythia-160m\n","EleutherAI/pythia-160m-deduped\n","EleutherAI/pythia-1b\n","EleutherAI/pythia-1b-deduped\n","EleutherAI/pythia-2.8b\n","EleutherAI/pythia-2.8b-deduped\n","EleutherAI/pythia-31m\n","EleutherAI/pythia-410m\n","EleutherAI/pythia-410m-deduped\n","EleutherAI/pythia-6.9b\n","EleutherAI/pythia-6.9b-deduped\n","EleutherAI/pythia-70m\n","EleutherAI/pythia-70m-deduped\n","garage-bAInd/Camel-Platypus2-13B\n","garage-bAInd/Camel-Platypus2-70B\n","garage-bAInd/Platypus-30B\n","garage-bAInd/Platypus2-13B\n","garage-bAInd/Platypus2-70B\n","garage-bAInd/Platypus2-70B-instruct\n","garage-bAInd/Platypus2-7B\n","garage-bAInd/Stable-Platypus2-13B\n","google/codegemma-7b-it\n","google/gemma-2b\n","google/gemma-2b-it\n","google/gemma-7b\n","google/gemma-7b-it\n","h2oai/h2o-danube2-1.8b-chat\n","keeeeenw/MicroLlama\n","lmsys/longchat-13b-16k\n","lmsys/longchat-7b-16k\n","lmsys/vicuna-13b-v1.3\n","lmsys/vicuna-13b-v1.5\n","lmsys/vicuna-13b-v1.5-16k\n","lmsys/vicuna-33b-v1.3\n","lmsys/vicuna-7b-v1.3\n","lmsys/vicuna-7b-v1.5\n","lmsys/vicuna-7b-v1.5-16k\n","meta-llama/Llama-2-13b-chat-hf\n","meta-llama/Llama-2-13b-hf\n","meta-llama/Llama-2-70b-chat-hf\n","meta-llama/Llama-2-70b-hf\n","meta-llama/Llama-2-7b-chat-hf\n","meta-llama/Llama-2-7b-hf\n","meta-llama/Meta-Llama-3-70B\n","meta-llama/Meta-Llama-3-70B-Instruct\n","meta-llama/Meta-Llama-3-8B\n","meta-llama/Meta-Llama-3-8B-Instruct\n","microsoft/phi-1_5\n","microsoft/phi-2\n","microsoft/Phi-3-mini-4k-instruct\n","mistralai/Mistral-7B-Instruct-v0.1\n","mistralai/Mistral-7B-Instruct-v0.2\n","mistralai/Mistral-7B-Instruct-v0.3\n","mistralai/Mistral-7B-v0.1\n","mistralai/Mistral-7B-v0.3\n","mistralai/Mixtral-8x7B-Instruct-v0.1\n","mistralai/Mixtral-8x7B-v0.1\n","NousResearch/Nous-Hermes-13b\n","NousResearch/Nous-Hermes-llama-2-7b\n","NousResearch/Nous-Hermes-Llama2-13b\n","openlm-research/open_llama_13b\n","openlm-research/open_llama_3b\n","openlm-research/open_llama_7b\n","stabilityai/FreeWilly2\n","stabilityai/stable-code-3b\n","stabilityai/stablecode-completion-alpha-3b\n","stabilityai/stablecode-completion-alpha-3b-4k\n","stabilityai/stablecode-instruct-alpha-3b\n","stabilityai/stablelm-3b-4e1t\n","stabilityai/stablelm-base-alpha-3b\n","stabilityai/stablelm-base-alpha-7b\n","stabilityai/stablelm-tuned-alpha-3b\n","stabilityai/stablelm-tuned-alpha-7b\n","stabilityai/stablelm-zephyr-3b\n","tiiuae/falcon-180B\n","tiiuae/falcon-180B-chat\n","tiiuae/falcon-40b\n","tiiuae/falcon-40b-instruct\n","tiiuae/falcon-7b\n","tiiuae/falcon-7b-instruct\n","TinyLlama/TinyLlama-1.1B-Chat-v1.0\n","TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n","togethercomputer/LLaMA-2-7B-32K\n","togethercomputer/RedPajama-INCITE-7B-Base\n","togethercomputer/RedPajama-INCITE-7B-Chat\n","togethercomputer/RedPajama-INCITE-7B-Instruct\n","togethercomputer/RedPajama-INCITE-Base-3B-v1\n","togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n","togethercomputer/RedPajama-INCITE-Chat-3B-v1\n","togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n","togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n","togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n","Trelis/Llama-2-7b-chat-hf-function-calling-v2\n","unsloth/Mistral-7B-v0.2\n"]}],"source":["!litgpt download list"]},{"cell_type":"markdown","id":"2495037e-0068-49ad-9bed-0bcdc440727d","metadata":{"id":"2495037e-0068-49ad-9bed-0bcdc440727d"},"source":["- We can then download an LLM via the following command"]},{"cell_type":"code","execution_count":3,"id":"FtRk8PJtV6CA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266740,"status":"ok","timestamp":1720559851839,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"FtRk8PJtV6CA","outputId":"d0663790-06bd-4474-e661-9beca0161c3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["repo_id: microsoft/Phi-3-mini-4k-instruct\n","Setting HF_HUB_ENABLE_HF_TRANSFER=1\n","/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n","For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n","  warnings.warn(\n","model-00001-of-00002.safetensors: 100%|█████| 4.97G/4.97G [00:27<00:00, 181MB/s]\n","model-00002-of-00002.safetensors: 100%|█████| 2.67G/2.67G [00:08<00:00, 331MB/s]\n","Converting .safetensor files to PyTorch binaries (.bin)\n","checkpoints/microsoft/Phi-3-mini-4k-instruct/model-00002-of-00002.safetensors --> checkpoints/microsoft/Phi-3-mini-4k-instruct/model-00002-of-00002.bin\n","checkpoints/microsoft/Phi-3-mini-4k-instruct/model-00001-of-00002.safetensors --> checkpoints/microsoft/Phi-3-mini-4k-instruct/model-00001-of-00002.bin\n","Converting checkpoint files to LitGPT format.\n","{'checkpoint_dir': PosixPath('checkpoints/microsoft/Phi-3-mini-4k-instruct'),\n"," 'debug_mode': False,\n"," 'dtype': None,\n"," 'model_name': None}\n","Loading weights: model-00002-of-00002.bin: 100%|████████| 00:10<00:00,  9.61it/s\n","Saving converted checkpoint to checkpoints/microsoft/Phi-3-mini-4k-instruct\n"]}],"source":["!litgpt download microsoft/Phi-3-mini-4k-instruct"]},{"cell_type":"code","execution_count":null,"id":"fb0c202d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216769,"status":"ok","timestamp":1720560068598,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"fb0c202d","outputId":"c103431d-5b15-4d44-bfd0-59d08076c5c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["repo_id: microsoft/phi-2\n","Setting HF_HUB_ENABLE_HF_TRANSFER=1\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n","For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n","  warnings.warn(\n","config.json: 100% 735/735 [00:00<00:00, 2.83MB/s]\n","generation_config.json: 100% 124/124 [00:00<00:00, 466kB/s]\n","model-00001-of-00002.safetensors: 100% 5.00G/5.00G [00:55<00:00, 89.9MB/s]\n","model-00002-of-00002.safetensors: 100% 564M/564M [00:04<00:00, 126MB/s]\n","model.safetensors.index.json: 100% 35.7k/35.7k [00:00<00:00, 21.1MB/s]\n","tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 35.8MB/s]\n","tokenizer_config.json: 100% 7.34k/7.34k [00:00<00:00, 24.4MB/s]\n","Converting .safetensor files to PyTorch binaries (.bin)\n","checkpoints/microsoft/phi-2/model-00002-of-00002.safetensors --> checkpoints/microsoft/phi-2/model-00002-of-00002.bin\n","checkpoints/microsoft/phi-2/model-00001-of-00002.safetensors --> checkpoints/microsoft/phi-2/model-00001-of-00002.bin\n","Converting checkpoint files to LitGPT format.\n","{'checkpoint_dir': PosixPath('checkpoints/microsoft/phi-2'),\n"," 'debug_mode': False,\n"," 'dtype': None,\n"," 'model_name': None}\n","Loading weights: model-00001-of-00002.bin:   0% 0/100 [00:00<?, ?it/s][W init.cpp:1436] Warning: write_record(): Passing Storage by data pointer is deprecated and will be an error in the future, please pass the Storage object instead. (function operator())\n","Loading weights: model-00002-of-00002.bin: 100% 100.0/100 [00:58<00:00,  1.71it/s]\n","Saving converted checkpoint to checkpoints/microsoft/phi-2\n"]}],"source":["!litgpt download microsoft/phi-2"]},{"cell_type":"code","execution_count":null,"id":"cRYS-WmdZ6ax","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"cRYS-WmdZ6ax","outputId":"689a677e-ff64-4e14-d236-6c92ef902373"},"outputs":[{"name":"stdout","output_type":"stream","text":["repo_id: EleutherAI/pythia-410m-deduped\n","Setting HF_HUB_ENABLE_HF_TRANSFER=1\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n","For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n","  warnings.warn(\n","config.json: 100% 570/570 [00:00<00:00, 2.43MB/s]\n","pytorch_model.bin: 100% 911M/911M [00:24<00:00, 37.4MB/s]\n","tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 23.0MB/s]\n","tokenizer_config.json: 100% 396/396 [00:00<00:00, 1.72MB/s]\n","Converting checkpoint files to LitGPT format.\n","{'checkpoint_dir': PosixPath('checkpoints/EleutherAI/pythia-410m-deduped'),\n"," 'debug_mode': False,\n"," 'dtype': None,\n"," 'model_name': None}\n","Loading weights: pytorch_model.bin:   0% 0/100 [00:00<?, ?it/s][W init.cpp:1436] Warning: write_record(): Passing Storage by data pointer is deprecated and will be an error in the future, please pass the Storage object instead. (function operator())\n","Loading weights: pytorch_model.bin: 100% 100.0/100 [00:22<00:00,  4.52it/s]\n","Saving converted checkpoint to checkpoints/EleutherAI/pythia-410m-deduped\n"]}],"source":["# This model did not perform well, I don't recommend using it\n","# !litgpt download EleutherAI/pythia-410m-deduped"]},{"cell_type":"markdown","id":"6caf5be0-4aa1-498f-b08a-68ff234cbea5","metadata":{"id":"6caf5be0-4aa1-498f-b08a-68ff234cbea5"},"source":["- And there's also a Python API to use the model"]},{"cell_type":"code","execution_count":null,"id":"e057edbf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"elapsed":462276,"status":"ok","timestamp":1720560788708,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"e057edbf","outputId":"58e09357-7e63-4beb-d785-e0251242e5f6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/litgpt/utils.py:559: UserWarning: The file size of checkpoints/microsoft/phi-2/lit_model.pth is over 4.2 GB. Using a model with more than 1B parameters on a CPU can be slow, it is recommended to switch to a GPU.\n","  warnings.warn(\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" Answer: Newton's laws of motion are three principles that describe the relationship between the forces acting on an object and its motion. \\n\\nThe first law states that an object at rest will remain at rest and an object in motion will remain in motion\""]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["from litgpt import LLM\n","\n","llm = LLM.load(\"microsoft/phi-2\")\n","# llm = LLM.load(\"microsoft/Phi-3-mini-4k-instruct\")\n","\n","llm.generate(\"Explain Newton's laws of motion with examples.\")"]},{"cell_type":"code","execution_count":31,"id":"WmpgYXUWcnaG","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":96385,"status":"ok","timestamp":1720561433398,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"WmpgYXUWcnaG","outputId":"7a3bc90e-f5b6-46ce-bf06-08865f16896b"},"outputs":[{"data":{"text/plain":["\"Newton's laws of motion, formulated by Sir Isaac Newton, are three fundamental laws that describe the relationship between a body and the forces acting upon it. These laws have been essential in advancing the field of classical mechanics.\\n\\n\\n\""]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["from litgpt import LLM\n","\n","# llm = LLM.load(\"microsoft/phi-2\")\n","llm = LLM.load(\"microsoft/Phi-3-mini-4k-instruct\")\n","\n","llm.generate(\"Explain Newton's laws of motion with examples.\")"]},{"cell_type":"code","execution_count":32,"id":"fc775d4e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":895},"executionInfo":{"elapsed":191682,"status":"error","timestamp":1720561625062,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"fc775d4e","outputId":"ffd1998f-a8cd-42a8-d34c-795bcc31023b"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Newton's laws of motion describe the relationship between a body and the forces acting upon it, and the body's motion in response to those forces. They are three physical laws that together laid the foundation for classical mechanics.\n","\n","\n","1. Newton's First Law (Law of Inertia) states that an object at rest will stay at rest, and an object in motion will stay in motion at a constant velocity, unless acted upon by a net external force. This principle means that there is a natural tendency of objects to keep moving in a straight line at a constant speed or to remain still.\n","\n","\n","   Example: Consider a hockey puck sliding on a smooth ice surface. If no external forces like friction or another player's stick apply force on the puck, it will continue to slide indefinitely in the same direction at a constant speed.\n","\n","\n","2. Newton's Second Law of Motion states that the acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass. The direction of the acceleration is in the direction of the applied net force. This law is often written as F = ma, where F is the net force applied, m is the mass of the object, and a is the acceleration.\n","\n","\n","\n","\n","   Example: Imagine pushing a shopping cart with a force of 10 Newtons in the direction you are pushing"]}],"source":["result = llm.generate(\n","    \"Explain Newton's laws of motion with examples.\",\n","    stream=True,\n","    max_new_tokens=512\n",")\n","for e in result:\n","    print(e, end=\"\", flush=True)"]},{"cell_type":"markdown","id":"288158da","metadata":{"id":"288158da"},"source":["---\n","\n","# Exercise 2: Download an LLM"]},{"cell_type":"markdown","id":"2717b67f","metadata":{"id":"2717b67f"},"source":["- Download and try out an LLM of your own choice (recommendation: 7B parameters or smaller)\n","- We will finetune the LLM in the next notebook\n","- You can also try out the `litgpt chat` command from the terminal"]},{"cell_type":"code","execution_count":null,"id":"10d13509","metadata":{},"outputs":[],"source":["# Run this in a terminal (without the \"!\")\n","!litgpt chat \"microsoft/Phi-3-mini-4k-instruct\""]},{"cell_type":"code","execution_count":null,"id":"01087aed","metadata":{},"outputs":[],"source":["# You can also try quantizing. This runs 2x tokens/second, but\n","# the accuracy is slightly lower, with typos and occassional\n","# repetitive phrases.\n","!litgpt chat --quantize=\"bnb.nf4\" \"microsoft/Phi-3-mini-4k-instruct\""]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/rasbt/LLM-workshop-2024/blob/main/05_weightloading/05_part-1.ipynb","timestamp":1720557677242}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":5}

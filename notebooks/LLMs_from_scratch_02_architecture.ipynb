{"cells":[{"cell_type":"markdown","id":"08f4321d-d32a-4a90-bfc7-e923f316b2f8","metadata":{"id":"08f4321d-d32a-4a90-bfc7-e923f316b2f8"},"source":["**LLM Workshop 2024 by Sebastian Raschka**\n","\n","This code is based on *Build a Large Language Model (From Scratch)*, [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)"]},{"cell_type":"markdown","id":"ce9295b2-182b-490b-8325-83a67c4a001d","metadata":{"id":"ce9295b2-182b-490b-8325-83a67c4a001d"},"source":["# 3) Coding an LLM architecture"]},{"cell_type":"code","execution_count":1,"id":"qNhatW3l5nyH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8611,"status":"ok","timestamp":1720548307345,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"qNhatW3l5nyH","outputId":"55b2c7b0-fa69-47ac-8c77-63854c57ebc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# Requirements from: https://github.com/rasbt/LLM-workshop-2024/blob/main/requirements.txt\n","requirements = \"\"\"\n","# torch >= 2.0.1\n","tiktoken >= 0.5.1\n","# matplotlib >= 3.7.1\n","# numpy >= 1.24.3\n","# tensorflow >= 2.15.0\n","# tqdm >= 4.66.1\n","# numpy >= 1.25, < 2.0\n","# pandas >= 2.2.1\n","# psutil >= 5.9.5\n","# litgpt[all] >= 0.4.1\n","\"\"\"\n","\n","with open(\"requirements.txt\", mode=\"wt\") as f:\n","    f.write(requirements)\n","\n","%pip install -r requirements.txt --quiet"]},{"cell_type":"code","execution_count":2,"id":"6KFw-81yqT2k","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":173,"status":"ok","timestamp":1720548321455,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"6KFw-81yqT2k","outputId":"d36e521b-1b7d-4251-caac-296c842f61d2"},"outputs":[{"data":{"text/plain":["ModuleSpec(name='tiktoken', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f41e3129e70>, origin='/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tiktoken/__init__.py', submodule_search_locations=['/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tiktoken'])"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import importlib\n","\n","importlib.util.find_spec(\"tiktoken\")"]},{"cell_type":"code","execution_count":3,"id":"f9eac223-a125-40f7-bacc-bd0d890450c7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1720548329974,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"f9eac223-a125-40f7-bacc-bd0d890450c7","outputId":"61733875-03ca-4cc0-e644-9c39e2496efd"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch version: 2.2.1+cu121\n","tiktoken version: 0.7.0\n"]}],"source":["from importlib.metadata import version\n","\n","\n","print(\"torch version:\", version(\"torch\"))\n","print(\"tiktoken version:\", version(\"tiktoken\"))"]},{"cell_type":"markdown","id":"vgtRAPoN6ixW","metadata":{"id":"vgtRAPoN6ixW"},"source":["Add supplementary Python module from Sebastian Raschka's training material"]},{"cell_type":"code","execution_count":4,"id":"C45nepme6Waq","metadata":{"executionInfo":{"elapsed":521,"status":"ok","timestamp":1720548344959,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"C45nepme6Waq"},"outputs":[],"source":["import requests\n","\n","session = requests.Session()\n","with open(\"llm_architecture.py\", \"wt\", encoding=\"utf-8\") as f:\n","    response = session.get(\n","        \"https://raw.githubusercontent.com/rasbt/LLM-workshop-2024/main/03_architecture/supplementary.py\"\n","    )\n","    f.write(response.text)"]},{"cell_type":"markdown","id":"e7da97ed-e02f-4d7f-b68e-a0eba3716e02","metadata":{"id":"e7da97ed-e02f-4d7f-b68e-a0eba3716e02"},"source":["- In this notebook, we implement a GPT-like LLM architecture; the next notebook will focus on training this LLM"]},{"cell_type":"markdown","id":"7d4f11e0-4434-4979-9dee-e1207df0eb01","metadata":{"id":"7d4f11e0-4434-4979-9dee-e1207df0eb01"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/figures/01.png?raw=1\" width=\"1000px\">"]},{"cell_type":"markdown","id":"53fe99ab-0bcf-4778-a6b5-6db81fb826ef","metadata":{"id":"53fe99ab-0bcf-4778-a6b5-6db81fb826ef"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# 3.1 Coding an LLM architecture"]},{"cell_type":"markdown","id":"ad72d1ff-d82d-4e33-a88e-3c1a8831797b","metadata":{"id":"ad72d1ff-d82d-4e33-a88e-3c1a8831797b"},"source":["- Models like GPT, Gemma, Phi, Mistral, Llama etc. generate words sequentially and are based on the decoder part of the original transformer architecture\n","- Therefore, these LLMs are often referred to as \"decoder-like\" LLMs\n","- Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code\n","- We'll see that many elements are repeated in an LLM's architecture"]},{"cell_type":"markdown","id":"5c5213e9-bd1c-437e-aee8-f5e8fb717251","metadata":{"id":"5c5213e9-bd1c-437e-aee8-f5e8fb717251"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/figures/02.png?raw=1\" width=\"700px\">"]},{"cell_type":"markdown","id":"0d43f5e2-fb51-434a-b9be-abeef6b98d99","metadata":{"id":"0d43f5e2-fb51-434a-b9be-abeef6b98d99"},"source":["- In the previous notebook, we used small embedding dimensions for token inputs and outputs for ease of illustration, ensuring they neatly fit on the screen\n","- In this notebook, we consider embedding and model sizes akin to a small GPT-2 model\n","- We'll specifically code the architecture of the smallest GPT-2 model (124 million parameters), as outlined in Radford et al.'s [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (note that the initial report lists it as 117M parameters, but this was later corrected in the model weight repository)\n"]},{"cell_type":"markdown","id":"3b3fc01b-3d69-4b74-bc89-c9ab472842ea","metadata":{"id":"3b3fc01b-3d69-4b74-bc89-c9ab472842ea"},"source":["\n","<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/figures/03.png?raw=1\" width=\"1200px\">\n","\n","- The next notebook will show how to load pretrained weights into our implementation, which will be compatible with model sizes of 345, 762, and 1542 million parameters\n","- Models like Llama and others are very similar to this model, since they are all based on the same core concepts\n","\n","<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/figures/04.png?raw=1\" width=\"1200px\">"]},{"cell_type":"markdown","id":"21baa14d-24b8-4820-8191-a2808f7fbabc","metadata":{"id":"21baa14d-24b8-4820-8191-a2808f7fbabc"},"source":["- Configuration details for the 124 million parameter GPT-2 model (GPT-2 \"small\") include:"]},{"cell_type":"code","execution_count":5,"id":"5ed66875-1f24-445d-add6-006aae3c5707","metadata":{"executionInfo":{"elapsed":171,"status":"ok","timestamp":1720550057895,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"5ed66875-1f24-445d-add6-006aae3c5707"},"outputs":[],"source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,  # Vocabulary size\n","    \"context_length\": 1024,  # Context length\n","    \"emb_dim\": 768,  # Embedding dimension\n","    \"n_heads\": 12,  # Number of attention heads\n","    \"n_layers\": 12,  # Number of layers\n","    \"drop_rate\": 0.0,  # Dropout rate\n","    \"qkv_bias\": False,  # Query-Key-Value bias\n","}"]},{"cell_type":"markdown","id":"46618527-15ac-4c32-ad85-6cfea83e006e","metadata":{"id":"46618527-15ac-4c32-ad85-6cfea83e006e"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","\n","\n","# 3.2 Coding the GPT model"]},{"cell_type":"markdown","id":"dec7d03d-9ff3-4ca3-ad67-01b67c2f5457","metadata":{"id":"dec7d03d-9ff3-4ca3-ad67-01b67c2f5457"},"source":["- We are almost there: now let's plug in the transformer block into the architecture we coded at the very beginning of this notebook so that we obtain a useable GPT architecture\n","- Note that the transformer block is repeated multiple times; in the case of the smallest 124M GPT-2 model, we repeat it 12 times:"]},{"cell_type":"markdown","id":"9b7b362d-f8c5-48d2-8ebd-722480ac5073","metadata":{"id":"9b7b362d-f8c5-48d2-8ebd-722480ac5073"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/figures/07.png?raw=1\" width=\"800px\">"]},{"cell_type":"markdown","id":"324e4b5d-ed89-4fdf-9a52-67deee0593bc","metadata":{"id":"324e4b5d-ed89-4fdf-9a52-67deee0593bc"},"source":["- The corresponding code implementation, where `cfg[\"n_layers\"] = 12`:"]},{"cell_type":"code","execution_count":6,"id":"c61de39c-d03c-4a32-8b57-f49ac3834857","metadata":{"executionInfo":{"elapsed":4754,"status":"ok","timestamp":1720551049117,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"c61de39c-d03c-4a32-8b57-f49ac3834857"},"outputs":[],"source":["import torch.nn as nn\n","from llm_architecture import TransformerBlock, LayerNorm\n","\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n","        )\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits"]},{"cell_type":"markdown","id":"2750270f-c45d-4410-8767-a6adbd05d5c3","metadata":{"id":"2750270f-c45d-4410-8767-a6adbd05d5c3"},"source":["- Using the configuration of the 124M parameter model, we can now instantiate this GPT model with random initial weights as follows:"]},{"cell_type":"code","execution_count":7,"id":"9bf6abb6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2121,"status":"ok","timestamp":1720551186017,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"9bf6abb6","outputId":"bcbb883c-d6e1-4c4b-f9f0-011b88cc14c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n"]}],"source":["import torch\n","import tiktoken\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","batch = []\n","\n","txt1 = \"Every effort moves you\"\n","txt2 = \"Every day holds a\"\n","\n","batch.append(torch.tensor(tokenizer.encode(txt1)))\n","batch.append(torch.tensor(tokenizer.encode(txt2)))\n","batch = torch.stack(batch, dim=0)\n","print(batch)"]},{"cell_type":"code","execution_count":8,"id":"ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2613,"status":"ok","timestamp":1720551207333,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64","outputId":"374f7e40-e546-4e10-dede-c3ad68b0d6dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input batch:\n"," tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n","\n","Output shape: torch.Size([2, 4, 50257])\n","tensor([[[ 6.4165e-02,  2.0443e-01, -1.6945e-01,  ...,  1.7887e-01,\n","           2.1921e-01, -5.8153e-01],\n","         [ 3.7736e-01, -4.2545e-01, -6.5874e-01,  ..., -2.5050e-01,\n","           4.6553e-01, -2.5760e-01],\n","         [ 8.8996e-01, -1.3770e-01,  1.4748e-01,  ...,  1.7770e-01,\n","          -1.2015e-01, -1.8902e-01],\n","         [-9.7276e-01,  9.7338e-02, -2.5419e-01,  ...,  1.1035e+00,\n","           3.7639e-01, -5.9006e-01]],\n","\n","        [[ 6.4165e-02,  2.0443e-01, -1.6945e-01,  ...,  1.7887e-01,\n","           2.1921e-01, -5.8153e-01],\n","         [ 1.3433e-01, -2.1289e-01, -2.7021e-02,  ...,  8.1153e-01,\n","          -4.7410e-02,  3.1186e-01],\n","         [ 8.9996e-01,  9.5396e-01, -1.7896e-01,  ...,  8.3053e-01,\n","           2.7657e-01, -2.4577e-02],\n","         [-9.3013e-05,  1.9390e-01,  5.1217e-01,  ...,  1.1915e+00,\n","          -1.6431e-01,  3.7046e-02]]], grad_fn=<UnsafeViewBackward0>)\n"]}],"source":["torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","\n","out = model(batch)\n","print(\"Input batch:\\n\", batch)\n","print(\"\\nOutput shape:\", out.shape)\n","print(out)"]},{"cell_type":"code","execution_count":9,"id":"0ad3f71f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6109\n","3626\n","6100\n","345\n","6109\n","1110\n","6622\n","257\n"]}],"source":["for i, input_txt in enumerate(tokenizer.encode(txt1) + tokenizer.encode(txt2)):\n","    print(input_txt)"]},{"cell_type":"code","execution_count":10,"id":"300e9d3d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Currently, the untrained model does not produce meaningful outputs:\n","\n","|     input     |    output     |\n","|---------------|---------------|\n","| Every         |  Orche        |\n","|  effort       |  compan       |\n","|  moves        | Friday        |\n","|  you          |  Ae           |\n","| Every         |  Orche        |\n","|  day          |  Dre          |\n","|  holds        |  Valent       |\n","|  a            | ftime         |\n","|---------------|---------------|\n"]}],"source":["print(\"Currently, the untrained model does not produce meaningful outputs:\\n\")\n","\n","print(f\"|{'input':^15}|{'output':^15}|\")\n","print(\"|\" + \"-\" * 15 + \"|\" + \"-\" * 15 + \"|\")\n","output_ids = torch.argmax(out, dim=2).view(-1, 1).tolist()\n","for i, input_txt in enumerate(tokenizer.encode(txt1) + tokenizer.encode(txt2)):\n","    print(\n","        f\"| {tokenizer.decode([input_txt]):<14}| {tokenizer.decode(output_ids[i]):<14}|\"\n","    )\n","print(\"|\" + \"-\" * 15 + \"|\" + \"-\" * 15 + \"|\")"]},{"cell_type":"markdown","id":"44a1bb67-be42-431d-87d0-00c005f4a520","metadata":{"id":"44a1bb67-be42-431d-87d0-00c005f4a520"},"source":["- We will train this model in the next notebook"]},{"cell_type":"markdown","id":"da5d9bc0-95ab-45d4-9378-417628d86e35","metadata":{"id":"da5d9bc0-95ab-45d4-9378-417628d86e35"},"source":["# 3.4 Generating text"]},{"cell_type":"markdown","id":"48da5deb-6ee0-4b9b-8dd2-abed7ed65172","metadata":{"id":"48da5deb-6ee0-4b9b-8dd2-abed7ed65172"},"source":["- LLMs like the GPT model we implemented above are used to generate one word at a time"]},{"cell_type":"markdown","id":"caade12a-fe97-480f-939c-87d24044edff","metadata":{"id":"caade12a-fe97-480f-939c-87d24044edff"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/figures/08.png?raw=1\" width=\"600px\">"]},{"cell_type":"markdown","id":"a7061524-a3bd-4803-ade6-2e3b7b79ac13","metadata":{"id":"a7061524-a3bd-4803-ade6-2e3b7b79ac13"},"source":["- The following `generate_text_simple` function implements greedy decoding, which is a simple and fast method to generate text\n","- In greedy decoding, at each step, the model chooses the word (or token) with the highest probability as its next output (the highest logit corresponds to the highest probability, so we technically wouldn't even have to compute the softmax function explicitly)\n","- The figure below depicts how the GPT model, given an input context, generates the next word token"]},{"cell_type":"markdown","id":"7ee0f32c-c18c-445e-b294-a879de2aa187","metadata":{"id":"7ee0f32c-c18c-445e-b294-a879de2aa187"},"source":["<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/figures/09.png?raw=1\" width=\"900px\">"]},{"cell_type":"code","execution_count":46,"id":"2de0edb2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Temperature: 1.0\n","Tensor: {tensor([0.1001, 0.2002, 0.4004], dtype=torch.bfloat16)}\n","Softmax: tensor([0.2891, 0.3203, 0.3906], dtype=torch.bfloat16)\n","w/ temp: tensor([0.2891, 0.3203, 0.3906], dtype=torch.bfloat16)\n","Temperature: 0.9\n","Tensor: {tensor([0.1001, 0.2002, 0.4004], dtype=torch.bfloat16)}\n","Softmax: tensor([0.2891, 0.3203, 0.3906], dtype=torch.bfloat16)\n","w/ temp: tensor([0.2852, 0.3184, 0.3965], dtype=torch.bfloat16)\n","Temperature: 0.5\n","Tensor: {tensor([0.1001, 0.2002, 0.4004], dtype=torch.bfloat16)}\n","Softmax: tensor([0.2891, 0.3203, 0.3906], dtype=torch.bfloat16)\n","w/ temp: tensor([0.2471, 0.3027, 0.4512], dtype=torch.bfloat16)\n","Temperature: 0.1\n","Tensor: {tensor([0.1001, 0.2002, 0.4004], dtype=torch.bfloat16)}\n","Softmax: tensor([0.2891, 0.3203, 0.3906], dtype=torch.bfloat16)\n","w/ temp: tensor([0.0420, 0.1143, 0.8438], dtype=torch.bfloat16)\n"]}],"source":["def sample_temperature(\n","    a: torch.Tensor = torch.tensor([0.1, 0.2, 0.4], dtype=torch.bfloat16),\n","    temperature: float = 0.9,\n","    seed: int | None = None\n","):\n","    if seed:\n","        # Store the current state to restore it later\n","        random_state = torch.random.get_rng_state()\n","        torch.manual_seed(seed)\n","    print(\"Temperature:\", temperature)\n","    print(\"Tensor:\", {a})\n","    print(\"Softmax:\", torch.softmax(a, dim=-1))\n","    print(\"w/ temp:\", torch.softmax((a / temperature), dim=-1))\n","    # Restore the random number generator\n","    if seed:\n","        torch.random.set_rng_state(random_state)\n","    return None\n","\n","sample_temperature(temperature=1.0)\n","sample_temperature(temperature=0.9)\n","sample_temperature(temperature=0.5)\n","sample_temperature(temperature=0.1)"]},{"cell_type":"code","execution_count":47,"id":"c9b428a9-8764-4b36-80cd-7d4e00595ba6","metadata":{"executionInfo":{"elapsed":188,"status":"ok","timestamp":1720551555570,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"c9b428a9-8764-4b36-80cd-7d4e00595ba6"},"outputs":[],"source":["def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx"]},{"cell_type":"code","execution_count":117,"id":"eeee2825","metadata":{},"outputs":[],"source":["def generate_text_with_temperature(\n","    model,\n","    idx,\n","    max_new_tokens: int,\n","    context_size: int,\n","    top_k: int | None = None,\n","    top_p: float | None = None,\n","    temperature: float = 0.9,\n","    seed: int | None = None,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Generate text using the provided model and temperature values.\n","\n","    Parameters\n","    ----------\n","    model: PyTorch model\n","        Transformer model that outputs logits for a given input.\n","    idx: torch.Tensor of shape: (batch_size, n_tokens)\n","        Input tensor of token indices.\n","    max_new_tokens: int\n","        The maximum number of new tokens to generate.\n","    context_size: int\n","        The model's context size\n","    top_k: int, default = None\n","        If provided, filter the logits based on the top_k highest values\n","        prior to sampling.\n","    top_p: float in the range [0.0, 1.0], default = None\n","        If provided, filter the logits based on the top_p percent values\n","        prior to sampling.\n","    temperature: float in the range [0.0, 1.0] default = 0.9\n","        Add sampling variety. Higher values increase sampling variety.\n","        1.0 means greedy search (argmax).\n","    seed: int, default = None\n","        If provided, makes the output deterministic.\n","\n","    Returns\n","    -------\n","    output_indices: a tensor of output indices\n","\n","    References\n","    ---------\n","    - [LabML: Sampling with Temperature](https://nn.labml.ai/sampling/temperature.html)\n","    - [torch.distributions.Categorical](https://pytorch.org/docs/stable/distributions.html#categorical)\n","    - [torch.multinomial](https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial), which is equivalent to torch...Categorical\n","    \"\"\"\n","    if seed:\n","        # Store the current state to restore it later\n","        random_state = torch.random.get_rng_state()\n","        torch.manual_seed(seed)\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply top-k and top-p filtering\n","        if top_k:\n","            top_k_idx = torch.argsort(input=logits, dim=-1, descending=True)[:, :top_k]\n","            filter_idx = top_k_idx\n","        \n","        if top_p:\n","            top_p_vals = torch.quantile(input=logits, q=top_p, dim=-1)\n","            top_p_idx = torch.argwhere(logits >= top_p_vals)\n","            # Reshape to match the input: batch, num_top_p_indices\n","            top_p_idx = top_p_idx[:, 1].view(logits.shape[0], -1)\n","            filter_idx = top_p_idx\n","\n","        # Filter using the most restrictive criteria\n","        if top_k and top_p:\n","            if top_p_idx.shape[1] < top_k:\n","                # Use top_p since it is more restrictive\n","                filter_idx = top_p_idx\n","            else:\n","                # Use top_k since it is more restrictive\n","                filter_idx = top_k_idx\n","        \n","        # Set to -inf for the softmax\n","        if top_k or top_p:\n","            print(\"Logits index shape\", logits.shape)\n","            print(\"Filter index shape\", filter_idx.shape)\n","            logits_copy = torch.ones_like(logits) * (-torch.inf)\n","            logits_copy[:, filter_idx.squeeze()] = logits[:, filter_idx.squeeze()]\n","            logits = logits_copy\n","\n","        # Apply softmax to get probabilities\n","        if temperature == 0.0:\n","            temperature = 1e-5  # to avoid ZeroDivision errors\n","        probs = torch.softmax(logits / temperature, dim=-1)  # (batch, vocab_size)\n","\n","        # Sample using temperature:\n","        if temperature < 1.0:\n","            # Adapted from: https://huggingface.co/transformers/v3.4.0/_modules/transformers/generation_utils.html\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","        else:\n","            # Get the idx of the vocab entry with the highest probability value\n","            idx_next = torch.argmax(probs, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","    \n","    # Restore the random number generator\n","    if seed:\n","        torch.random.set_rng_state(random_state)\n","    \n","    return idx\n"]},{"cell_type":"code","execution_count":98,"id":"321393d6","metadata":{},"outputs":[],"source":["a = torch.rand((4, 100))\n","\n","top_k = 10\n","top_p = 0.95\n","\n","if top_k:\n","    top_k_idx = torch.argsort(input=a, dim=-1, descending=True)[:, :top_k]\n","\n","if top_p:\n","    top_p_vals = torch.quantile(input=a, q=top_p, dim=-1).view(-1, 1)\n","    top_p_idx = torch.argwhere(a >= top_p_vals)\n","    # Reshape to match the input: batch, num_top_p_indices\n","    top_p_idx = top_p_idx[:, 1].view(a.shape[0], -1)\n","\n","if top_k and top_p:\n","    if top_p_idx.shape[0] < top_k:\n","        # Use top_p since it is more restrictive\n","        pass\n","    else:\n","        # Use top_k since it is more restrictive\n","        pass"]},{"cell_type":"code","execution_count":94,"id":"fc9f1e33","metadata":{},"outputs":[{"data":{"text/plain":["tensor(True)"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["# Different ways to get the values that are greater than a given probability\n","torch.all(\n","    a[a >= top_p_vals] == a[\n","        torch.argwhere(a >= top_p_vals)[:, 0],\n","        torch.argwhere(a >= top_p_vals)[:, 1]\n","    ]\n",")"]},{"cell_type":"code","execution_count":103,"id":"077809c3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[36, 42, 50, 51, 89],\n","        [17, 18, 34, 42, 70],\n","        [24, 48, 63, 76, 98],\n","        [36, 39, 76, 96, 97]])\n","tensor([[36, 89, 50, 51, 42, 33, 62, 78, 15, 28],\n","        [42, 34, 70, 17, 18, 95, 75, 41, 13, 33],\n","        [48, 24, 76, 63, 98, 15, 77, 61, 12,  3],\n","        [96, 36, 76, 97, 39, 79, 57, 83, 98, 75]])\n"]}],"source":["# Notice that the index values in top_p correspond\n","# to those in top_k (just not the same order)\n","print(top_p_idx)\n","print(top_k_idx)"]},{"cell_type":"markdown","id":"6515f2c1-3cc7-421c-8d58-cc2f563b7030","metadata":{"id":"6515f2c1-3cc7-421c-8d58-cc2f563b7030"},"source":["- The `generate_text_simple` above implements an iterative process, where it creates one token at a time\n","\n","<img src=\"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/figures/10.png?raw=1\" width=\"800px\">"]},{"cell_type":"markdown","id":"b0fa8b2c-4d97-4259-a8da-8ffb6bb088be","metadata":{"id":"b0fa8b2c-4d97-4259-a8da-8ffb6bb088be"},"source":["# Exercise: Generate some text"]},{"cell_type":"markdown","id":"f682eac4-f9bd-438b-9dec-6b1cc7bc05ce","metadata":{"id":"f682eac4-f9bd-438b-9dec-6b1cc7bc05ce"},"source":["1. Use the `tokenizer.encode` method to prepare some input text\n","2. Then, convert this text into a pytprch tensor via (`torch.tensor`)\n","3. Add a batch dimension via `.unsqueeze(0)`\n","4. Use the `generate_text_simple` function to have the GPT generate some text based on your prepared input text\n","5. The output from step 4 will be token IDs, convert them back into text via the `tokenizer.decode` method"]},{"cell_type":"code","execution_count":49,"id":"2286f6de-5222-46f8-ad0d-d1f380a36636","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":805,"status":"ok","timestamp":1720552200968,"user":{"displayName":"Ryan Parker","userId":"18317737450322819970"},"user_tz":300},"id":"2286f6de-5222-46f8-ad0d-d1f380a36636","outputId":"fffb36db-11fa-444a-ca02-f8a2119d9ba0"},"outputs":[{"name":"stdout","output_type":"stream","text":["40________I\n","588_______ like\n","284_______ to\n","804_______ look\n","329_______ for\n","6290______ rain\n","25435_____bows\n","28800_____ comprises\n","24739_____ GH\n","7267______ argue\n","49240_____ decaying\n","26594_____fighting\n"]}],"source":["text = \"I like to look for rainbows\"\n","tokens = tokenizer.encode(text)  # convert text to tokens\n","tokens = torch.tensor(tokens).unsqueeze(0)  # convert to tensor and add batch dimension\n","model.eval()  # disable dropout\n","output = generate_text_simple(\n","    model=model, idx=tokens, max_new_tokens=5, context_size=1024\n",")\n","for one_token in output.squeeze():\n","    print(f\"{one_token}\".ljust(10, \"_\"), tokenizer.decode([one_token]), sep=\"\")"]},{"cell_type":"code","execution_count":56,"id":"c32a2ee7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["40________I\n","588_______ like\n","284_______ to\n","804_______ look\n","329_______ for\n","6290______ rain\n","25435_____bows\n","28800_____ comprises\n","24739_____ GH\n","7267______ argue\n","49240_____ decaying\n","26594_____fighting\n"]}],"source":["text = \"I like to look for rainbows\"\n","tokens = tokenizer.encode(text)  # convert text to tokens\n","tokens = torch.tensor(tokens).unsqueeze(0)  # convert to tensor and add batch dimension\n","model.eval()  # disable dropout\n","output = generate_text_with_temperature(\n","    model=model,\n","    idx=tokens,\n","    max_new_tokens=5,\n","    context_size=1024,\n","    top_k = None,\n","    top_p = None,\n","    temperature = 1.0,  # reproduce the results above\n","    seed = None,\n",")\n","for one_token in output.squeeze():\n","    print(f\"{one_token}\".ljust(10, \"_\"), tokenizer.decode([one_token]), sep=\"\")"]},{"cell_type":"code","execution_count":107,"id":"fa7fbc81","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["40________I\n","588_______ like\n","284_______ to\n","804_______ look\n","329_______ for\n","6290______ rain\n","25435_____bows\n","13076_____ bust\n","4258______ climate\n","1904______use\n","1824______ustom\n","34955_____ fishermen\n"]}],"source":["text = \"I like to look for rainbows\"\n","tokens = tokenizer.encode(text)  # convert text to tokens\n","tokens = torch.tensor(tokens).unsqueeze(0)  # convert to tensor and add batch dimension\n","model.eval()  # disable dropout\n","output = generate_text_with_temperature(\n","    model=model,\n","    idx=tokens,\n","    max_new_tokens=5,\n","    context_size=1024,\n","    top_k = None,\n","    top_p = None,\n","    temperature = 0.9,  # add sample variability\n","    seed = None,\n",")\n","for one_token in output.squeeze():\n","    print(f\"{one_token}\".ljust(10, \"_\"), tokenizer.decode([one_token]), sep=\"\")"]},{"cell_type":"code","execution_count":119,"id":"9ee2f04f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Logits index shape torch.Size([1, 50257])\n","Filter index shape torch.Size([1, 10])\n","Logits index shape torch.Size([1, 50257])\n","Filter index shape torch.Size([1, 10])\n","Logits index shape torch.Size([1, 50257])\n","Filter index shape torch.Size([1, 10])\n","Logits index shape torch.Size([1, 50257])\n","Filter index shape torch.Size([1, 10])\n","Logits index shape torch.Size([1, 50257])\n","Filter index shape torch.Size([1, 10])\n","40________I\n","588_______ like\n","284_______ to\n","804_______ look\n","329_______ for\n","6290______ rain\n","25435_____bows\n","10712_____ tissue\n","32769_____ guardians\n","38679_____ocl\n","4194______ agg\n","16081_____ vib\n"]}],"source":["text = \"I like to look for rainbows\"\n","tokens = tokenizer.encode(text)  # convert text to tokens\n","tokens = torch.tensor(tokens).unsqueeze(0)  # convert to tensor and add batch dimension\n","model.eval()  # disable dropout\n","output = generate_text_with_temperature(\n","    model=model,\n","    idx=tokens,\n","    max_new_tokens=5,\n","    context_size=1024,\n","    top_k = 10,\n","    top_p = 0.95,\n","    temperature = 0.9,  # add sample variability\n","    seed = None,\n",")\n","for one_token in output.squeeze():\n","    print(f\"{one_token}\".ljust(10, \"_\"), tokenizer.decode([one_token]), sep=\"\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/rasbt/LLM-workshop-2024/blob/main/03_architecture/03.ipynb","timestamp":1720535359046}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":5}
